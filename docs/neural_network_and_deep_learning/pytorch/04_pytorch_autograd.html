<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.43">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Deep Learning Notes – pytorch_autograd</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Deep Learning Notes</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../neural_network_and_deep_learning/week_2/w2_binary_classification.html"> 
<span class="menu-text">Lecture notes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#deriving-the-gradient" id="toc-deriving-the-gradient" class="nav-link active" data-scroll-target="#deriving-the-gradient">Deriving the Gradient</a></li>
  <li><a href="#putting-it-all-together" id="toc-putting-it-all-together" class="nav-link" data-scroll-target="#putting-it-all-together">Putting It All Together</a></li>
  <li><a href="#gradient-flow-in-classification-problems" id="toc-gradient-flow-in-classification-problems" class="nav-link" data-scroll-target="#gradient-flow-in-classification-problems">Gradient Flow in Classification Problems</a>
  <ul class="collapse">
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#cross-entropy-loss-function" id="toc-cross-entropy-loss-function" class="nav-link" data-scroll-target="#cross-entropy-loss-function">Cross-Entropy Loss Function</a></li>
  <li><a href="#softmax-output" id="toc-softmax-output" class="nav-link" data-scroll-target="#softmax-output">Softmax Output</a></li>
  <li><a href="#gradient-flow" id="toc-gradient-flow" class="nav-link" data-scroll-target="#gradient-flow">Gradient Flow</a>
  <ul class="collapse">
  <li><a href="#gradient-with-respect-to-the-softmax-output" id="toc-gradient-with-respect-to-the-softmax-output" class="nav-link" data-scroll-target="#gradient-with-respect-to-the-softmax-output">Gradient with Respect to the Softmax Output</a></li>
  <li><a href="#gradient-with-respect-to-the-logits" id="toc-gradient-with-respect-to-the-logits" class="nav-link" data-scroll-target="#gradient-with-respect-to-the-logits">Gradient with Respect to the Logits</a></li>
  <li><a href="#gradient-with-respect-to-weights-and-biases" id="toc-gradient-with-respect-to-weights-and-biases" class="nav-link" data-scroll-target="#gradient-with-respect-to-weights-and-biases">Gradient with Respect to Weights and Biases</a></li>
  </ul></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary">Summary</a></li>
  <li><a href="#now-we-will-see-how-local-gradient-is-calculated-for-linear-transformations-of-neural-networks." id="toc-now-we-will-see-how-local-gradient-is-calculated-for-linear-transformations-of-neural-networks." class="nav-link" data-scroll-target="#now-we-will-see-how-local-gradient-is-calculated-for-linear-transformations-of-neural-networks.">Now we will see how local gradient is calculated for linear transformations of neural networks.</a>
  <ul class="collapse">
  <li><a href="#formulation-of-the-problem" id="toc-formulation-of-the-problem" class="nav-link" data-scroll-target="#formulation-of-the-problem">Formulation of the Problem</a></li>
  <li><a href="#local-gradient" id="toc-local-gradient" class="nav-link" data-scroll-target="#local-gradient">Local Gradient</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">




<div id="cell-0" class="cell" data-execution_count="64">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> ipywidgets <span class="im">as</span> widgets</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.autograd <span class="im">as</span> autograd</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> ipywidgets <span class="im">import</span> fixed  <span class="co"># Importing fixed</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-1" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_multiple_functions(func_list,x_range_start<span class="op">=-</span><span class="dv">10</span>, x_range_end<span class="op">=</span><span class="dv">10</span>,  num_points<span class="op">=</span><span class="dv">100</span>, log_scale<span class="op">=</span><span class="va">False</span>, <span class="op">**</span>kwargs):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> np.linspace(x_range_start, x_range_end, num_points)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> func <span class="kw">in</span> func_list:</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> func(x)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>        plt.plot(x, y, label<span class="op">=</span><span class="ss">f'</span><span class="sc">{</span>func<span class="sc">.</span><span class="va">__name__</span><span class="sc">}</span><span class="ss">'</span>, <span class="op">**</span>kwargs)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> log_scale:</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>        plt.yscale(<span class="st">'log'</span>)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">'x'</span>)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">'y'</span>)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">'Plot of Multiple Functions'</span>)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    plt.grid(<span class="va">True</span>)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    plt.legend()</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-2" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co">## Lets start with a simple scalar example</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigmoid(x):</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> torch.exp(<span class="op">-</span>x))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-3" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Widget to adjust x_range_start</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>x_range_start_slider <span class="op">=</span> widgets.FloatSlider(<span class="bu">min</span><span class="op">=-</span><span class="dv">20</span>, <span class="bu">max</span><span class="op">=</span><span class="dv">0</span>, step<span class="op">=</span><span class="fl">0.5</span>, value<span class="op">=-</span><span class="dv">10</span>, description<span class="op">=</span><span class="st">'x_range_start'</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Interactive function</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>widgets.interact(plot_multiple_functions, </span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>                 func_list<span class="op">=</span>fixed([sigmoid]), </span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>                 x_range_start<span class="op">=</span>x_range_start_slider, </span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>                 x_range_end<span class="op">=</span>fixed(<span class="dv">10</span>), </span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>                 num_points<span class="op">=</span>fixed(<span class="dv">100</span>), </span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>                 log_scale<span class="op">=</span>fixed(<span class="va">False</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"7b5ca2262b934dfc8de27a54ff52b0f9","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display" data-execution_count="26">
<pre><code>&lt;function __main__.plot_multiple_functions(func_list, x_range_start=-10, x_range_end=10, num_points=100, log_scale=False, **kwargs)&gt;</code></pre>
</div>
</div>
<div id="cell-4" class="cell" data-execution_count="80">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>my_test_linear_transform <span class="op">=</span> torch.nn.Linear(<span class="dv">5</span>,<span class="dv">3</span>)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, param <span class="kw">in</span> my_test_linear_transform.named_parameters():</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(name, param)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>weight Parameter containing:
tensor([[ 0.4286,  0.3634,  0.3349, -0.0226,  0.3046],
        [-0.1771, -0.4120, -0.4424,  0.0497, -0.0045],
        [-0.1930, -0.1363, -0.0225,  0.2749, -0.0244]], requires_grad=True)
bias Parameter containing:
tensor([ 0.2026, -0.1183,  0.1449], requires_grad=True)</code></pre>
</div>
</div>
<div id="cell-5" class="cell" data-execution_count="95">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co">## lets create a sample input dataset. Input is 10 samples with 5 features. Output is 10 samples with 3 classes</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">10</span>,<span class="dv">5</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>y_target <span class="op">=</span> torch.randint(<span class="dv">0</span>,<span class="dv">3</span>,(<span class="dv">10</span>,))</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>x.requires_grad <span class="op">=</span> <span class="va">False</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>weight_matrix <span class="op">=</span> my_test_linear_transform.weight.detach().clone()</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>weight_matrix.requires_grad <span class="op">=</span> <span class="va">False</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>bias <span class="op">=</span> my_test_linear_transform.bias.detach().clone()</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>bias.requires_grad <span class="op">=</span> <span class="va">False</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a><span class="co">## lets create a simple linear and compute loss</span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>    y_a_manual <span class="op">=</span> my_test_linear_transform(x)</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>    y_a_manual_mm <span class="op">=</span> x <span class="op">@</span> weight_matrix.t() <span class="op">+</span> bias</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>    y_a_softmax_manual <span class="op">=</span> F.softmax(y_a_manual, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>    loss_manual_vec <span class="op">=</span> F.cross_entropy(y_a_manual, y_target, reduction<span class="op">=</span><span class="st">'none'</span>)</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>    loss_manual <span class="op">=</span> loss_manual_vec.mean()</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-6" class="cell" data-execution_count="96">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># # Example tensors</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="co"># matrix_tensor = torch.randn(10, 5)  # A (10, 5) tensor</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="co"># index_tensor = torch.randint(0, 5, (10,))  # An index tensor of shape (10) with values between 0 and 4</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="co"># # Extract values</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="co"># extracted_values = matrix_tensor[torch.arange(matrix_tensor.size(0)), index_tensor]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>To derive the gradient of the softmax activation function, let’s start with the definition of the softmax function. The softmax function is often used in multi-class classification tasks, and it is defined as follows:</p>
<p>For a vector <span class="math inline">\(\mathbf{z} = [z_1, z_2, \dots, z_n]\)</span>, the softmax function produces an output vector <span class="math inline">\(\mathbf{y} = [y_1, y_2, \dots, y_n]\)</span>, where each component <span class="math inline">\(y_i\)</span> is given by:</p>
<p><span class="math display">\[
y_i = \frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}}
\]</span></p>
<p>Here, <span class="math inline">\(\mathbf{y}\)</span> represents the probabilities for each class, and each <span class="math inline">\(y_i\)</span> is constrained to lie between 0 and 1, with the sum of all <span class="math inline">\(y_i\)</span> equal to 1.</p>
<section id="deriving-the-gradient" class="level3">
<h3 class="anchored" data-anchor-id="deriving-the-gradient">Deriving the Gradient</h3>
<p>The goal is to find the gradient of the softmax function with respect to its inputs <span class="math inline">\(\mathbf{z}\)</span>. Specifically, we want to compute <span class="math inline">\(\frac{\partial y_i}{\partial z_k}\)</span> for each pair of <span class="math inline">\(i\)</span> and <span class="math inline">\(k\)</span>.</p>
<p><strong>Case 1: When <span class="math inline">\(i = k\)</span></strong></p>
<p>Let’s start with the derivative of <span class="math inline">\(y_i\)</span> with respect to <span class="math inline">\(z_i\)</span>:</p>
<p><span class="math display">\[
\frac{\partial y_i}{\partial z_i} = \frac{\partial}{\partial z_i} \left(\frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}}\right)
\]</span></p>
<p>Using the quotient rule:</p>
<p><span class="math display">\[
\frac{\partial y_i}{\partial z_i} = \frac{ \left(\sum_{j=1}^{n} e^{z_j}\right) \cdot \frac{\partial e^{z_i}}{\partial z_i} - e^{z_i} \cdot \frac{\partial \sum_{j=1}^{n} e^{z_j}}{\partial z_i}}{\left(\sum_{j=1}^{n} e^{z_j}\right)^2}
\]</span></p>
<p>Since <span class="math inline">\(\frac{\partial e^{z_i}}{\partial z_i} = e^{z_i}\)</span> and <span class="math inline">\(\frac{\partial \sum_{j=1}^{n} e^{z_j}}{\partial z_i} = e^{z_i}\)</span>, we have:</p>
<p><span class="math display">\[
\frac{\partial y_i}{\partial z_i} = \frac{e^{z_i} \sum_{j=1}^{n} e^{z_j} - e^{z_i} \cdot e^{z_i}}{\left(\sum_{j=1}^{n} e^{z_j}\right)^2}
\]</span></p>
<p><span class="math display">\[
\frac{\partial y_i}{\partial z_i} = \frac{e^{z_i} \left(\sum_{j=1}^{n} e^{z_j} - e^{z_i}\right)}{\left(\sum_{j=1}^{n} e^{z_j}\right)^2}
\]</span></p>
<p><span class="math display">\[
\frac{\partial y_i}{\partial z_i} = \frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}} \cdot \left(1 - \frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}}\right)
\]</span></p>
<p><span class="math display">\[
\frac{\partial y_i}{\partial z_i} = y_i \cdot (1 - y_i)
\]</span></p>
<p><strong>Case 2: When <span class="math inline">\(i \neq k\)</span></strong></p>
<p>Now, consider the derivative of <span class="math inline">\(y_i\)</span> with respect to <span class="math inline">\(z_k\)</span> where <span class="math inline">\(i \neq k\)</span>:</p>
<p><span class="math display">\[
\frac{\partial y_i}{\partial z_k} = \frac{\partial}{\partial z_k} \left(\frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}}\right)
\]</span></p>
<p>Again, using the quotient rule:</p>
<p><span class="math display">\[
\frac{\partial y_i}{\partial z_k} = \frac{0 \cdot \sum_{j=1}^{n} e^{z_j} - e^{z_i} \cdot \frac{\partial \sum_{j=1}^{n} e^{z_j}}{\partial z_k}}{\left(\sum_{j=1}^{n} e^{z_j}\right)^2}
\]</span></p>
<p>Since <span class="math inline">\(\frac{\partial \sum_{j=1}^{n} e^{z_j}}{\partial z_k} = e^{z_k}\)</span>, we have:</p>
<p><span class="math display">\[
\frac{\partial y_i}{\partial z_k} = - \frac{e^{z_i} \cdot e^{z_k}}{\left(\sum_{j=1}^{n} e^{z_j}\right)^2}
\]</span></p>
<p><span class="math display">\[
\frac{\partial y_i}{\partial z_k} = - \frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}} \cdot \frac{e^{z_k}}{\sum_{j=1}^{n} e^{z_j}}
\]</span></p>
<p><span class="math display">\[
\frac{\partial y_i}{\partial z_k} = - y_i \cdot y_k
\]</span></p>
</section>
<section id="putting-it-all-together" class="level3">
<h3 class="anchored" data-anchor-id="putting-it-all-together">Putting It All Together</h3>
<p>The gradient of the softmax function with respect to its inputs can be expressed as:</p>
<p><span class="math display">\[
\frac{\partial y_i}{\partial z_k} =
\begin{cases}
y_i \cdot (1 - y_i) &amp; \text{if } i = k \\
- y_i \cdot y_k &amp; \text{if } i \neq k
\end{cases}
\]</span></p>
<p>This result can also be written more compactly using the Kronecker delta notation <span class="math inline">\(\delta_{ik}\)</span>, which is 1 if <span class="math inline">\(i = k\)</span> and 0 otherwise:</p>
<p><span class="math display">\[
\frac{\partial y_i}{\partial z_k} = y_i \cdot (\delta_{ik} - y_k)
\]</span></p>
<p>This is the gradient of the softmax function with respect to its inputs, and it’s crucial for calculating gradients in neural networks during backpropagation when using softmax as the output activation function.</p>
</section>
<section id="gradient-flow-in-classification-problems" class="level1">
<h1>Gradient Flow in Classification Problems</h1>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>In a typical classification problem, we have:</p>
<ul>
<li><strong>N</strong>: The number of data points in the dataset.</li>
<li><strong>C</strong>: The number of classes or categories the model can predict.</li>
</ul>
<p>For each data point ( $ x_i $ ), the model outputs a probability distribution over the <strong>C</strong> classes using a softmax layer. The training process involves minimizing a loss function, often the cross-entropy loss, and updating the model’s parameters using gradient descent.</p>
</section>
<section id="cross-entropy-loss-function" class="level2">
<h2 class="anchored" data-anchor-id="cross-entropy-loss-function">Cross-Entropy Loss Function</h2>
<p>Given the true label ( $ y_i $ ) (one-hot encoded) and the predicted probability ( $ _i $ ) from the softmax layer, the cross-entropy loss for a single data point is:</p>
<p><span class="math display">\[
L_i = -\sum_{j=1}^{C} y_{ij} \log(\hat{y}_{ij})
\]</span></p>
<p>The total loss over all <strong>N</strong> data points is:</p>
<p><span class="math display">\[
L = -\frac{1}{N} \sum_{i=1}^{N} \sum_{j=1}^{C} y_{ij} \log(\hat{y}_{ij})
\]</span></p>
</section>
<section id="softmax-output" class="level2">
<h2 class="anchored" data-anchor-id="softmax-output">Softmax Output</h2>
<p>The softmax function converts the logits (raw output of the network) ( $ z_i $ ) into probabilities ( $ _i $ ) for each class:</p>
<p><span class="math display">\[
\hat{y}_{ij} = \frac{e^{z_{ij}}}{\sum_{k=1}^{C} e^{z_{ik}}}
\]</span></p>
<p>Where: - ( $ z_{ij }$ ) is the logit for the ( $ j $ )-th class of the ( i )-th data point. - ( <span class="math inline">\(\hat{y}_{ij}\)</span> ) is the predicted probability of the ( i )-th data point belonging to class ( j ).</p>
</section>
<section id="gradient-flow" class="level2">
<h2 class="anchored" data-anchor-id="gradient-flow">Gradient Flow</h2>
<section id="gradient-with-respect-to-the-softmax-output" class="level3">
<h3 class="anchored" data-anchor-id="gradient-with-respect-to-the-softmax-output">Gradient with Respect to the Softmax Output</h3>
<p>The gradient of the loss function with respect to the softmax output ( <span class="math inline">\(\hat{y}_{ij}\)</span> ) is:</p>
<p><span class="math display">\[
\frac{\partial L}{\partial \hat{y}_{ij}} = -\frac{y_{ij}}{\hat{y}_{ij}}
\]</span></p>
</section>
<section id="gradient-with-respect-to-the-logits" class="level3">
<h3 class="anchored" data-anchor-id="gradient-with-respect-to-the-logits">Gradient with Respect to the Logits</h3>
<p>To compute the gradient with respect to the logits ( $ z_{ij} $ ), we apply the chain rule. The key steps are:</p>
<ol type="1">
<li><p><strong>Softmax Gradient</strong>:</p>
<ul>
<li><p>For ( i = j ):</p>
<p><span class="math display">\[
\frac{\partial \hat{y}_{ij}}{\partial z_{ij}} = \hat{y}_{ij}(1 - \hat{y}_{ij})
\]</span></p></li>
<li><p>For ( $i j $):</p>
<p><span class="math display">\[
\frac{\partial \hat{y}_{ik}}{\partial z_{ij}} = -\hat{y}_{ij}\hat{y}_{ik}
\]</span></p></li>
</ul></li>
<li><p><strong>Chain Rule Application</strong>:</p>
<p>The gradient of the loss with respect to the logits is:</p>
<p><span class="math display">\[
\frac{\partial L}{\partial z_{ij}} = \hat{y}_{ij} - y_{ij}
\]</span></p>
<p>This result shows that the gradient is the difference between the predicted probability ( $ <em>{ij} $ ) and the true label ( $ y</em>{ij} $ ).</p></li>
</ol>
</section>
<section id="gradient-with-respect-to-weights-and-biases" class="level3">
<h3 class="anchored" data-anchor-id="gradient-with-respect-to-weights-and-biases">Gradient with Respect to Weights and Biases</h3>
<p>Assume the logits ( $ z_{ij} $ ) are computed as a linear combination of input features ( $ x_i $ ) with weights ( $ w_j $ ) and bias ( $ b_j $ ):</p>
<p><span class="math display">\[
z_{ij} = w_j^T x_i + b_j
\]</span></p>
<p>The gradients with respect to the weights and biases are:</p>
<ul>
<li><p><strong>Weights</strong>:</p>
<p><span class="math display">\[
\frac{\partial L}{\partial w_j} = \sum_{i=1}^{N} \frac{\partial L}{\partial z_{ij}} \cdot \frac{\partial z_{ij}}{\partial w_j} = \sum_{i=1}^{N} (\hat{y}_{ij} - y_{ij}) x_i
\]</span></p></li>
<li><p><strong>Biases</strong>:</p>
<p><span class="math display">\[
\frac{\partial L}{\partial b_j} = \sum_{i=1}^{N} (\hat{y}_{ij} - y_{ij})
\]</span></p></li>
</ul>
<p>These gradients are then used to update the model’s parameters during the training process using gradient descent or a variant like stochastic gradient descent (SGD).</p>
</section>
</section>
<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">Summary</h2>
<ul>
<li>The gradient of the loss with respect to the logits ( $ z_{ij} $ ) is simply the difference between the predicted probability and the true label.</li>
<li>The gradients with respect to the weights and biases are derived using the chain rule and are used in the optimization process to minimize the loss function and improve the model’s performance.</li>
</ul>
<div id="cell-9" class="cell" data-execution_count="122">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.rand(<span class="dv">10</span>)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigmoid_f(x: torch.Tensor) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> torch.exp(<span class="op">-</span>x))</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> softmax_f(x: torch.Tensor) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    exp_x <span class="op">=</span> torch.exp(x)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> exp_x <span class="op">/</span> exp_x.<span class="bu">sum</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-10" class="cell" data-execution_count="121">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(sigmoid_f(x))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([0.5274, 0.6246, 0.5872, 0.5913, 0.5987, 0.5771, 0.5442, 0.5708, 0.5868,
        0.5979])</code></pre>
</div>
</div>
<div id="cell-11" class="cell" data-execution_count="123">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(sigmoid_f(x) <span class="op">*</span> (<span class="dv">1</span><span class="op">-</span> sigmoid_f(x)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([0.2293, 0.2044, 0.2483, 0.2175, 0.2471, 0.2340, 0.2267, 0.2224, 0.2499,
        0.2470])</code></pre>
</div>
</div>
<div id="cell-12" class="cell" data-execution_count="125">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(softmax_f(x) <span class="op">*</span> (<span class="dv">1</span><span class="op">-</span> softmax_f(x)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([0.0967, 0.1271, 0.0657, 0.1113, 0.0689, 0.0904, 0.1000, 0.1053, 0.0581,
        0.0690])</code></pre>
</div>
</div>
<div id="cell-13" class="cell" data-execution_count="127">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>torch.outer(softmax_f(x), softmax_f(x))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="127">
<pre><code>tensor([[0.0118, 0.0162, 0.0077, 0.0138, 0.0081, 0.0109, 0.0122, 0.0130, 0.0067,
         0.0081],
        [0.0162, 0.0223, 0.0106, 0.0191, 0.0111, 0.0150, 0.0168, 0.0179, 0.0093,
         0.0111],
        [0.0077, 0.0106, 0.0050, 0.0090, 0.0053, 0.0071, 0.0080, 0.0085, 0.0044,
         0.0053],
        [0.0138, 0.0191, 0.0090, 0.0163, 0.0095, 0.0128, 0.0144, 0.0153, 0.0079,
         0.0095],
        [0.0081, 0.0111, 0.0053, 0.0095, 0.0055, 0.0075, 0.0084, 0.0089, 0.0046,
         0.0056],
        [0.0109, 0.0150, 0.0071, 0.0128, 0.0075, 0.0101, 0.0113, 0.0120, 0.0062,
         0.0075],
        [0.0122, 0.0168, 0.0080, 0.0144, 0.0084, 0.0113, 0.0127, 0.0135, 0.0070,
         0.0084],
        [0.0130, 0.0179, 0.0085, 0.0153, 0.0089, 0.0120, 0.0135, 0.0143, 0.0074,
         0.0089],
        [0.0067, 0.0093, 0.0044, 0.0079, 0.0046, 0.0062, 0.0070, 0.0074, 0.0038,
         0.0046],
        [0.0081, 0.0111, 0.0053, 0.0095, 0.0056, 0.0075, 0.0084, 0.0089, 0.0046,
         0.0056]])</code></pre>
</div>
</div>
<div id="cell-14" class="cell" data-execution_count="124">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.autograd.functional <span class="im">import</span> jacobian, hessian</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.autograd <span class="im">import</span> grad</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>jacobian(sigmoid_f, x), jacobian(softmax_f, x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="124">
<pre><code>(tensor([[0.2293, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000],
         [0.0000, 0.2044, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000],
         [0.0000, 0.0000, 0.2483, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000],
         [0.0000, 0.0000, 0.0000, 0.2175, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.2471, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2340, 0.0000, 0.0000, 0.0000,
          0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267, 0.0000, 0.0000,
          0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2224, 0.0000,
          0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2499,
          0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.2470]]),
 tensor([[ 0.0967, -0.0162, -0.0077, -0.0138, -0.0081, -0.0109, -0.0122, -0.0130,
          -0.0067, -0.0081],
         [-0.0162,  0.1271, -0.0106, -0.0191, -0.0111, -0.0150, -0.0168, -0.0179,
          -0.0093, -0.0111],
         [-0.0077, -0.0106,  0.0657, -0.0090, -0.0053, -0.0071, -0.0080, -0.0085,
          -0.0044, -0.0053],
         [-0.0138, -0.0191, -0.0090,  0.1113, -0.0095, -0.0128, -0.0144, -0.0153,
          -0.0079, -0.0095],
         [-0.0081, -0.0111, -0.0053, -0.0095,  0.0689, -0.0075, -0.0084, -0.0089,
          -0.0046, -0.0056],
         [-0.0109, -0.0150, -0.0071, -0.0128, -0.0075,  0.0904, -0.0113, -0.0120,
          -0.0062, -0.0075],
         [-0.0122, -0.0168, -0.0080, -0.0144, -0.0084, -0.0113,  0.1000, -0.0135,
          -0.0070, -0.0084],
         [-0.0130, -0.0179, -0.0085, -0.0153, -0.0089, -0.0120, -0.0135,  0.1053,
          -0.0074, -0.0089],
         [-0.0067, -0.0093, -0.0044, -0.0079, -0.0046, -0.0062, -0.0070, -0.0074,
           0.0581, -0.0046],
         [-0.0081, -0.0111, -0.0053, -0.0095, -0.0056, -0.0075, -0.0084, -0.0089,
          -0.0046,  0.0690]]))</code></pre>
</div>
</div>
</section>
<section id="now-we-will-see-how-local-gradient-is-calculated-for-linear-transformations-of-neural-networks." class="level2">
<h2 class="anchored" data-anchor-id="now-we-will-see-how-local-gradient-is-calculated-for-linear-transformations-of-neural-networks.">Now we will see how local gradient is calculated for linear transformations of neural networks.</h2>
<section id="formulation-of-the-problem" class="level3">
<h3 class="anchored" data-anchor-id="formulation-of-the-problem">Formulation of the Problem</h3>
<p>Given a weight matrix ( <span class="math inline">\(W\)</span> ) and a bias vector ( <span class="math inline">\(b\)</span> ), the output of a linear transformation is given by:</p>
<p><span class="math display">\[
z = Wx + b
\]</span></p>
<p>where: - ( <span class="math inline">\(W\)</span> ) is a ( $ m n $ ) matrix. - ( <span class="math inline">\(x\)</span> ) is a ($ n $ ) column vector. - ( <span class="math inline">\(b\)</span> ) is a ( $ m $) column vector. - ( <span class="math inline">\(z\)</span> ) is a ( $ m $) column vector.</p>
<p>At this point we should see matrix as a transformation of space. The matrix ( <span class="math inline">\(W\)</span> ) transforms the input vector ( <span class="math inline">\(x\)</span> ) from an ( $ n $ )-dimensional space to an ( $ m $ )-dimensional space. The bias vector ( <span class="math inline">\(b\)</span> ) shifts the transformed vector in the ( $ m $ )-dimensional space. The columns of the matrix ( <span class="math inline">\(W\)</span> ) are the directions in which the input vector ( <span class="math inline">\(x\)</span> ) is transformed. A matrix multiplication of a vector with the matrix ( <span class="math inline">\(W\)</span> ) is a linear combination of the columns of the matrix ( <span class="math inline">\(W\)</span> ) with the vector ( <span class="math inline">\(x\)</span> ) as the coefficients.</p>
<p><span class="math display">\[
W = \begin{bmatrix} | &amp; | &amp; | \\ w_1 &amp; w_2 &amp; w_3 \\ | &amp; | &amp; | \end{bmatrix} \\
W. \vec{x} = x1 * \vec{w_1} + x2 * \vec{w_2} + x3 * \vec{w_3}
\]</span></p>
<p>In terms of basis vectors, the matrix ( <span class="math inline">\(W\)</span> ) transforms the standard basis vectors ( $,, , $ ) to the columns of the matrix ( <span class="math inline">\(W\)</span> ). The output of the linear transformation is a linear combination of the transformed basis vectors.</p>
</section>
<section id="local-gradient" class="level3">
<h3 class="anchored" data-anchor-id="local-gradient">Local Gradient</h3>
<p>Lets derive the local gradient of linear transformation. This is a vector valued transformation. For a vector valued transformation, the local gradient is a Jacobian matrix. The Jacobian matrix is a matrix of partial derivatives of the output vector with respect to the input vector. Computing the Jacobian matrix involves computing the partial derivatives of each output element with respect to each input element.</p>
<p><span class="math display">\[
\begin{aligned}
z_i &amp; = \sum_{j=1}^{n} W_{ij}x_j + b_i \\
\frac{\partial z_i}{\partial x_j} &amp; = W_{ij} \\
\end{aligned}
\]</span></p>
<p>The Jacobian matrix is given by:</p>
<p><span class="math display">\[
\begin{aligned}
J &amp; = \begin{bmatrix} \frac{\partial z_1}{\partial x_1} &amp; \frac{\partial z_1}{\partial x_2} &amp; \ldots &amp; \frac{\partial z_1}{\partial x_n} \\ \frac{\partial z_2}{\partial x_1} &amp; \frac{\partial z_2}{\partial x_2} &amp; \ldots &amp; \frac{\partial z_2}{\partial x_n} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \frac{\partial z_m}{\partial x_1} &amp; \frac{\partial z_m}{\partial x_2} &amp; \ldots &amp; \frac{\partial z_m}{\partial x_n} \end{bmatrix} \\
\end{aligned}
\]</span></p>



</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>