[
  {
    "objectID": "neural_network_and_deep_learning/week_2/w2_binary_classification.html",
    "href": "neural_network_and_deep_learning/week_2/w2_binary_classification.html",
    "title": "Binary Classification",
    "section": "",
    "text": "Logistic Regression\nInput Feature matrix \\(X\\) and target vector \\(Y\\).\n\\(X = [x_1 ... x_{n_x}]\\)\nNB: \\(n_x\\) Represents the number of features. For example if its a colored image then the dimension of feature fector is 64 * 64 * 3\n\n\nNotations to be followed in this notebook\n\nObservations: \\((x,y)\\) where \\(x \\in \\mathbb{R}^{n_x}\\) and \\(y \\in {0,1}\\)\nNumber of training examples: \\(m\\)\n\\(m\\) Training examples are denoted by \\((x^{(1)},y^{(1)})...(x^{(m)},y^{(m)})\\)\n\\(M_{train}\\) is the number of examples training set and \\(M_{test}\\) is the number of examples in test set\n\\(X = [x^{(1)} ... x^{(m)}]\\) is the matrix of input features. Note we have m columns and n_x rows. where m represents the number of training examples and n_x represents the number of features.\n\n\n\n\n\n\n\nFeature Matrix\n\n\n\n\\[\nX_{n_{x} \\times m} =\n\\begin{pmatrix}\n    & & & \\\\\n    \\vdots & \\vdots & & \\vdots \\\\\n    x^{(1)} & x^{(2)} & \\cdots & x^{(m)} \\\\\n    \\vdots & \\vdots & & \\vdots \\\\\n    & & & \\\\\n\\end{pmatrix}\n\\] and \\(Y = [y^{(1)} ... y^{(m)}]\\) is the target vector where \\(Y \\in \\mathbb{R^{1 \\times m}}\\)\n\n\n\n\n\n\n\n\nNote\n\n\n\nWay we have defined the input feature matrix \\(X\\) is different from the way we define in traditional statistical courses of linear regression. In linear regression we have \\(X = [x_1 ... x_{n_x}]\\) where \\(x_i\\) is the feature vector of \\(i^{th}\\) feature. We formulate the problem as \\(Y_{m \\times 1} = X_{m \\times n_{x}}\\beta_{n_{x} \\times 1} + \\epsilon_{m\\times1}\\). \\(\\beta\\) estimates are formulated as \\(\\hat{\\beta} = (X^{T}X)^{-1}X^{T}Y\\). But here the convection we will follow is \\(X = [x^{(1)} ... x^{(m)}]\\) where \\(x^{(i)}\\) is the feature vector of \\(i^{th}\\) training example.",
    "crumbs": [
      "Lecture notes",
      "Neural Network and DL",
      "Neural Network Basics",
      "Binary Classification"
    ]
  },
  {
    "objectID": "neural_network_and_deep_learning/week_2/test_notebook.html",
    "href": "neural_network_and_deep_learning/week_2/test_notebook.html",
    "title": "this a notebook example",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np",
    "crumbs": [
      "Lecture notes",
      "Deep Learning",
      "this a notebook example"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to my lecture notes repository on Deep Learning (DL) and Machine Learning (ML). This site is designed to serve as a comprehensive resource for students, professionals, and enthusiasts interested in exploring the vast and exciting fields of DL and ML."
  },
  {
    "objectID": "about.html#who-am-i",
    "href": "about.html#who-am-i",
    "title": "About",
    "section": "Who Am I?",
    "text": "Who Am I?\nMy name is Sayan Dasgupta. I am currently working as a Principal Data Scientist, with a robust background in mathematics, classical statistics, and machine learning. As I continue to explore and delve deeper into the field of Deep Learning, I aim to leverage my knowledge and experience to share insights and foster learning within the community."
  },
  {
    "objectID": "about.html#purpose-of-this-site",
    "href": "about.html#purpose-of-this-site",
    "title": "About",
    "section": "Purpose of This Site",
    "text": "Purpose of This Site\nThis site is a curated collection of lecture notes, resources, and insights gathered from various courses, seminars, and self-study sessions. The primary aim is to provide a structured and easy-to-follow guide for anyone looking to understand and apply concepts in DL and ML. Whether you’re a beginner or someone looking to deepen your understanding, you’ll find valuable information here."
  },
  {
    "objectID": "about.html#what-youll-find-here",
    "href": "about.html#what-youll-find-here",
    "title": "About",
    "section": "What You’ll Find Here",
    "text": "What You’ll Find Here\n\nLecture Notes: Detailed notes from various courses and lectures on DL and ML topics, including but not limited to neural networks, deep learning algorithms, supervised and unsupervised learning, reinforcement learning, and more.\nAnimated Graphs: Visual aids created using Manim to help conceptualize and understand complex topics. These animations bring theoretical concepts to life, making them easier to grasp.\nCode Examples: Practical code snippets and examples, primarily using Python, to demonstrate how theoretical concepts are implemented in real-world scenarios.\nQuarto Projects: Interactive Quarto projects that you can explore and modify to enhance your learning experience.\nAdditional Resources: Links to research papers, articles, books, and online courses that I have found particularly useful."
  },
  {
    "objectID": "about.html#why-i-created-this-site",
    "href": "about.html#why-i-created-this-site",
    "title": "About",
    "section": "Why I Created This Site",
    "text": "Why I Created This Site\nI believe in the power of shared knowledge and the importance of making education accessible to all. By compiling and organizing my notes and resources, I hope to contribute to the learning community and provide a useful tool for anyone passionate about DL and ML."
  },
  {
    "objectID": "about.html#get-in-touch",
    "href": "about.html#get-in-touch",
    "title": "About",
    "section": "Get in Touch",
    "text": "Get in Touch\nI welcome feedback, questions, and collaboration opportunities. If you have any suggestions or would like to discuss anything related to the content on this site, feel free to reach out to me.\nThank you for visiting, and happy learning!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Hello from Sayan\nThis site serves as a curated collection of my understanding of different concepts in DL and ML, gathered from various courses, seminars, and self-study sessions. The primary aim is to provide a structured and easy-to-follow guide for anyone looking to understand and apply concepts in DL and ML. Whether you’re a beginner or someone looking to deepen your understanding, you’ll find valuable information here."
  },
  {
    "objectID": "neural_network_and_deep_learning/week_2/w2_gradient_descent.html",
    "href": "neural_network_and_deep_learning/week_2/w2_gradient_descent.html",
    "title": "Gradient Descent",
    "section": "",
    "text": "Our problem setup - We predict our $ = (w^t x + b) $. Defining \\(z = w^t x + b\\), $ = (z) $ - Our Cost function is \\(J(w,b) = \\frac{1}{m}\\sum_{i=1}^{m}L(y^{i},\\hat{y}^i)\\)\nWe need to minimise $ J(w,b) = - _{i=1}^{m} [y^i (^i) + (1 - y^i) (1 - ^i)] $\nA quick proof that our loss is convex",
    "crumbs": [
      "Lecture notes",
      "Neural Network and DL",
      "Neural Network Basics",
      "Gradient Descent"
    ]
  },
  {
    "objectID": "neural_network_and_deep_learning/week_2/w2_logistic_regression_cost_function.html",
    "href": "neural_network_and_deep_learning/week_2/w2_logistic_regression_cost_function.html",
    "title": "Logistic Regression Cost function",
    "section": "",
    "text": "Before we jump looking into cost function of logistic regression, lets quickly revise concavity and convexity of functions.",
    "crumbs": [
      "Lecture notes",
      "Neural Network and DL",
      "Neural Network Basics",
      "Logistic Regression Cost Function"
    ]
  },
  {
    "objectID": "neural_network_and_deep_learning/week_2/w2_logistic_regression_cost_function.html#quick-revisions",
    "href": "neural_network_and_deep_learning/week_2/w2_logistic_regression_cost_function.html#quick-revisions",
    "title": "Logistic Regression Cost function",
    "section": "",
    "text": "Before we jump looking into cost function of logistic regression, lets quickly revise concavity and convexity of functions.",
    "crumbs": [
      "Lecture notes",
      "Neural Network and DL",
      "Neural Network Basics",
      "Logistic Regression Cost Function"
    ]
  },
  {
    "objectID": "neural_network_and_deep_learning/week_2/w2_logistic_regression_cost_function.html#concave-function",
    "href": "neural_network_and_deep_learning/week_2/w2_logistic_regression_cost_function.html#concave-function",
    "title": "Logistic Regression Cost function",
    "section": "Concave function",
    "text": "Concave function\nA function is said to be concave if the line segment joining any two points on the graph of the function lies below the graph. In other words, a function is concave if its second derivative is negative everywhere. Mathematically, for a given function \\(f(x)\\), if \\(f''(x) &lt; 0\\) for all \\(x \\in domain of \\: f(x)\\), then \\(f(x)\\) is concave. Also given \\(\\lambda\\) and 2 points \\(x_1\\) and \\(x_2\\) on the graph of f(x), the following inequality holds true: \\[f(\\lambda x_1 + (1-\\lambda)x_2) \\geq \\lambda f(x_1) + (1-\\lambda)f(x_2)\\]\n\n## Here is an example of a concave function\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-10, 10, 100)\ndef f_concave(x):\n    return -x**2\n\nplt.figure(figsize=(6, 3)) \nplt.plot(x, f_concave(x))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProperties of Concave Functions\n\n\n\n\nSecond derivative is negative everywhere\nA concave function has a global maximum at the point where the first derivative is zero",
    "crumbs": [
      "Lecture notes",
      "Neural Network and DL",
      "Neural Network Basics",
      "Logistic Regression Cost Function"
    ]
  },
  {
    "objectID": "neural_network_and_deep_learning/week_2/w2_logistic_regression_cost_function.html#convex-function",
    "href": "neural_network_and_deep_learning/week_2/w2_logistic_regression_cost_function.html#convex-function",
    "title": "Logistic Regression Cost function",
    "section": "Convex function",
    "text": "Convex function\nA function is said to be convex if the line segment joining any two points on the graph of the function lies above the graph. In other words, a function is convex if its second derivative is positive everywhere. Mathematically, for a given function \\(f(x)\\), if \\(f''(x) &gt; 0\\) for all \\(x \\in domain of \\: f(x)\\), then \\(f(x)\\) is convex. Also given \\(\\lambda\\) and 2 points \\(x_1\\) and \\(x_2\\) on the graph of f(x), the following inequality holds true: \\[f(\\lambda x_1 + (1-\\lambda)x_2) \\leq \\lambda f(x_1) + (1-\\lambda)f(x_2)\\]\n\n## Here is an example of a convex function\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-10, 10, 100)\ndef f_convex(x):\n    return x**2\n\nplt.figure(figsize=(6, 3)) \nplt.plot(x, f_convex(x))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProperties of Convex Functions\n\n\n\n\nSecond derivative is positive everywhere\nA convex function has a global minimum at the point where the first derivative is zero\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the functions\ndef convex_function(x):\n    return x**2\n\ndef concave_function(x):\n    return -x**2\n\n# Generate x values\nx = np.linspace(-3, 3, 400)\n\n# Calculate y values for each function\ny_convex = convex_function(x)\ny_concave = concave_function(x)\n\n# Create the plots\n#plt.figure(figsize=(14, 6))\nplt.figure(figsize=(8, 3)) \n\n# Plot convex function\nplt.subplot(1, 2, 1)\nplt.plot(x, y_convex, label='f(x) = x^2')\nplt.scatter(0, convex_function(0), color='red', zorder=5)  # Global minimum at x = 0\nplt.title('Convex Function (Concave Up)')\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.legend()\nplt.grid(True)\n## add legend for the critical point\nplt.legend(['f(x) = x^2', 'critical point at x = 0'])\n\n# Plot concave function\nplt.subplot(1, 2, 2)\nplt.plot(x, y_concave, label='f(x) = -x^2')\nplt.scatter(0, concave_function(0), color='red', zorder=5)  # Global maximum at x = 0\nplt.title('Concave Function (Concave Down)')\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.legend()\nplt.legend(['f(x) = x^2', 'critical point at x = 0'])\n\nplt.grid(True)\nplt.tight_layout()\n\nplt.show()",
    "crumbs": [
      "Lecture notes",
      "Neural Network and DL",
      "Neural Network Basics",
      "Logistic Regression Cost Function"
    ]
  },
  {
    "objectID": "neural_network_and_deep_learning/week_2/w2_logistic_regression_cost_function.html#cost-function-of-logistic-regression",
    "href": "neural_network_and_deep_learning/week_2/w2_logistic_regression_cost_function.html#cost-function-of-logistic-regression",
    "title": "Logistic Regression Cost function",
    "section": "Cost Function of Logistic Regression",
    "text": "Cost Function of Logistic Regression\nQuick recap \\[ \\hat{y} = \\sigma(w^T x + b), \\: where \\: \\sigma(z) = 1/(1+ \\exp(-z))\\] Given observations \\(\\{ (x^{(1)},y^{(1)}, (x^{(2)},y^{(2)}, ..., (x^{(m)},y^{(m)}  ) \\}\\), we want \\(\\hat{y}^{(i)} = y^{(i)}\\)\nChoices of Loss function:\n\nMean Squared Error: \\(L(y,\\hat{y}) = (y-\\hat{y})^2\\)\nLogistic Loss: \\(L(y,\\hat{y}) = -[y \\log(\\hat{y}) + (1-y) \\log(1-\\hat{y})\\)]\n\nHow does the logistic regression loss behave?\nLets see through some plots\n\ndef logistic_loss(y,p):\n    return -y*np.log(p)-(1-y)*np.log(1-p)\n\np = np.linspace(0.001, 0.999, 1000)\n## logistic loss for y = 1\nloss_y1 = logistic_loss(1, p)\n## logistic loss for y = 0\nloss_y0 = logistic_loss(0, p)\n\nplt.figure(figsize=(8, 3))\nplt.plot(p, loss_y1, label='y=1')\nplt.plot(p, loss_y0, label='y=0')\nplt.xlabel('p (hat{y})')\nplt.ylabel('Logistic Loss')\nplt.legend()\nplt.title('Logistic Loss for y=1 and y=0')\nplt.grid(True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuick note\n\n\n\n\nSuppose \\(y=1\\), then \\(L(y,\\hat{y}) = -\\log(\\hat{y})\\) We are minimising the loss, hence we want to make \\(\\log(\\hat{y})\\) as large as possible and in turn \\(\\hat{y}\\) as large as possible\nSuppose \\(y=0\\), then \\(L(y,\\hat{y}) = -\\log(1-\\hat{y})\\) We are minimising the loss, hence we want to make \\(\\log(1-\\hat{y})\\) as large as possible and in turn we want to make \\(\\hat{y}\\) as small as possible",
    "crumbs": [
      "Lecture notes",
      "Neural Network and DL",
      "Neural Network Basics",
      "Logistic Regression Cost Function"
    ]
  },
  {
    "objectID": "neural_network_and_deep_learning/week_2/w2_logistic_regression_cost_function.html#cost-function",
    "href": "neural_network_and_deep_learning/week_2/w2_logistic_regression_cost_function.html#cost-function",
    "title": "Logistic Regression Cost function",
    "section": "Cost function",
    "text": "Cost function\nGiven observations \\(\\{ (x^{(1)},y^{(1)}, (x^{(2)},y^{(2)}, ..., (x^{(m)},y^{(m)}  ) \\}\\), we want \\(\\hat{y}^{(i)} = y^{(i)}\\)\nCost Function is defined as:\n\\[\\begin{align*}\nJ(w,b) &= \\frac{1}{m} \\sum_{i=1}^{m} L(y^{(i)},\\hat{y}^{(i)}) \\\\\n&= -\\frac{1}{m} \\sum_{i=1}^{m} [y^{(i)}\\log(\\hat{y}^{(i)}) + (1-y^{(i)})(1-\\log(\\hat{y}^{(i)}))]\n\\end{align*}\\]\nThis is nothing but the average of the logistic loss function over all the training examples. The goal is to find the values of \\(w\\) and \\(b\\) that minimizes the cost function \\(J(w,b)\\).",
    "crumbs": [
      "Lecture notes",
      "Neural Network and DL",
      "Neural Network Basics",
      "Logistic Regression Cost Function"
    ]
  },
  {
    "objectID": "neural_network_and_deep_learning/week_2/w2_logistic_regression.html",
    "href": "neural_network_and_deep_learning/week_2/w2_logistic_regression.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "We use this for binary classification problem. Given \\(x\\), we want \\(\\hat{y} = P(y = 1 | x)\\)\n\nx is a feature vector where \\(x \\in \\mathbb{R^{n_x}}\\) and b is a bias term \\(b \\in \\mathbb{R}\\)\nOne choice of modeling it will be to use a linear model: \\(\\hat{y} = (w^T x + b)\\) where \\(w \\in \\mathbb{R^{n_x}}\\)\n\nThe problem is ouput can range from \\(-\\infty\\) to \\(+\\infty\\) which is not what we want.\n\nNow consider the sigmoid function: \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\)\n\nThis function maps any real number to the range (0, 1)\nWe can use this function to model \\(P(y = 1 | x)\\) as \\(\\hat{y} = \\sigma(w^T x + b)\\)\n\n\n\nfrom manim import *\n\n\n# %%manim -qm -v WARNING SquareToCircle\n\n# class SquareToCircle(Scene):\n#    def construct(self):\n#       square = Square()\n#       circle = Circle()\n#       circle.set_fill(BLUE, opacity=0.5)\n#       self.play(Create(square))\n#       self.play(Transform(square, circle))\n#       self.wait()\n\n\n\n%%manim -qm -v WARNING LogisticLossSurface\n\nclass LogisticLossSurface(ThreeDScene):\n    def construct(self):\n        #self.set_camera_orientation(phi=75 * DEGREES, theta=30 * DEGREES)\n\n        # Define axes\n        # axes = ThreeDAxes(\n        #     x_range=[-3, 3, 1],\n        #     y_range=[-3, 3, 1],\n        #     z_range=[0, 100, 0.5],\n        #     x_length=7,\n        #     y_length=7,\n        #     z_length=4,\n        # )\n        axes = ThreeDAxes()\n        self.add(axes)\n\n        # Define logistic loss function\n        def logistic_loss(x, y):\n            z = 0.5 * x + 0.5 * y\n            p = 1 / (1 + np.exp(-z))\n            return -np.log(p)\n\n        def parabolic_func(x, y):\n            return 0.3 * x**2 + 0.5 * y**2\n\n\n        # Create the surface plot\n        surface = Surface(\n            lambda u, v: axes.c2p(u, v, parabolic_func(u, v)),\n            u_range=[-20, 20],\n            v_range=[-20, 20],\n            resolution=(50, 50),\n            fill_opacity=0.75\n        )\n        surface.set_color_by_gradient(BLUE, GREEN)\n        self.add(surface)\n\n        # Add labels\n        x_label = axes.get_x_axis_label(Tex(\"x\"), edge=RIGHT, direction=RIGHT).scale(0.7)\n        y_label = axes.get_y_axis_label(Tex(\"y\"), edge=UP, direction=UP).scale(0.7)\n        z_label = axes.get_z_axis_label(Tex(\"Loss\"), edge=OUT, direction=OUT).scale(0.7)\n        self.add(x_label, y_label, z_label)\n\n        # # Add interactive rotation\n        # self.begin_3dillusion_camera_rotation(rate=0.1)\n        # self.wait(10)\n        # self.stop_3dillusion_camera_rotation()\n        self.set_camera_orientation(phi=75 * DEGREES, theta=50 * DEGREES)\n        self.wait(2)\n\nManim Community v0.18.1\n\n\n\n\n\n      Your browser does not support the video element.\n    \n\n\n\n## Let us quickly check how the sigmoid function looks like\nimport matplotlib.pyplot as plt\nimport numpy as np\ndomain_def = np.arange(-10, 10, 0.1)\ndef sigmoid(x):\n    return 1/(1+np.exp(-x))\nrange_def = sigmoid(domain_def)\n\n# Create a new figure with a specific size (width, height)\nplt.figure(figsize=(6, 3))  # Adjust the height value as needed\n\nplt.plot(domain_def, range_def)\n## add a vertical line at x = 0\nplt.axvline(x=0, color='r')\nplt.scatter(0, 0.5, color='b')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nif \\(z = w^T x + b\\) is large and positive, then \\(\\sigma(z) = 1\\)+\nif \\(z = w^T x + b\\) is large and negative, then \\(\\sigma(z) = 0\\)\n\n\nfrom mpl_toolkits.mplot3d import Axes3D\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Step 1: Simulate the Data\nnp.random.seed(42)\nx = np.random.rand(100)\nepsilon = np.random.randn(100)\ny = 2 + 3 * x + epsilon\n\n# Generate a grid of alpha and beta values\nalpha_vals = np.linspace(-1, 5, 100)\nbeta_vals = np.linspace(0, 6, 100)\nalpha_grid, beta_grid = np.meshgrid(alpha_vals, beta_vals)\nloss_grid = np.zeros_like(alpha_grid)\n\n# Calculate the loss for each combination of alpha and beta\nfor i in range(len(alpha_vals)):\n    for j in range(len(beta_vals)):\n        alpha_temp = alpha_vals[i]\n        beta_temp = beta_vals[j]\n        y_pred_temp = alpha_temp + beta_temp * x\n        loss_grid[j, i] = np.mean((y - y_pred_temp) ** 2)\n\n# Plot the loss function in 3D\nfig = plt.figure(figsize=(12, 12))\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(alpha_grid, beta_grid, loss_grid, cmap='viridis')\n\nax.set_xlabel('Alpha')\nax.set_ylabel('Beta')\nax.set_zlabel('Loss (MSE)')\nax.set_title('Loss Function Surface')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nnp.random.rand(100)\n\narray([0.7948113 , 0.50263709, 0.57690388, 0.49251769, 0.19524299,\n       0.72245212, 0.28077236, 0.02431597, 0.6454723 , 0.17711068,\n       0.94045858, 0.95392858, 0.91486439, 0.3701587 , 0.01545662,\n       0.92831856, 0.42818415, 0.96665482, 0.96361998, 0.85300946,\n       0.29444889, 0.38509773, 0.85113667, 0.31692201, 0.16949275,\n       0.55680126, 0.93615477, 0.6960298 , 0.57006117, 0.09717649,\n       0.61500723, 0.99005385, 0.14008402, 0.51832965, 0.87737307,\n       0.74076862, 0.69701574, 0.70248408, 0.35949115, 0.29359184,\n       0.80936116, 0.81011339, 0.86707232, 0.91324055, 0.5113424 ,\n       0.50151629, 0.79829518, 0.64996393, 0.70196688, 0.79579267,\n       0.89000534, 0.33799516, 0.37558295, 0.09398194, 0.57828014,\n       0.03594227, 0.46559802, 0.54264463, 0.28654125, 0.59083326,\n       0.03050025, 0.03734819, 0.82260056, 0.36019064, 0.12706051,\n       0.52224326, 0.76999355, 0.21582103, 0.62289048, 0.08534746,\n       0.05168172, 0.53135463, 0.54063512, 0.6374299 , 0.72609133,\n       0.97585208, 0.51630035, 0.32295647, 0.79518619, 0.27083225,\n       0.43897142, 0.07845638, 0.02535074, 0.96264841, 0.83598012,\n       0.69597421, 0.40895294, 0.17329432, 0.15643704, 0.2502429 ,\n       0.54922666, 0.71459592, 0.66019738, 0.2799339 , 0.95486528,\n       0.73789692, 0.55435405, 0.61172075, 0.41960006, 0.24773099])\n\n\n\nx = np.random.rand(100)\n1 / (1 + np.exp(-2 - 3 * x))\n\narray([0.91901028, 0.98640724, 0.9792584 , 0.90915559, 0.9048505 ,\n       0.98374406, 0.90187989, 0.98863268, 0.98399511, 0.90413575,\n       0.90503912, 0.99303535, 0.95782399, 0.95738203, 0.98832312,\n       0.99216855, 0.99302209, 0.98607621, 0.95806435, 0.90469384,\n       0.98702205, 0.97528243, 0.9634791 , 0.99115539, 0.9116212 ,\n       0.97005166, 0.88432713, 0.96789096, 0.89742265, 0.91344584,\n       0.91313898, 0.98106508, 0.9857709 , 0.97702502, 0.99250888,\n       0.95789662, 0.94568639, 0.99010516, 0.93528072, 0.99253226,\n       0.88457266, 0.99267883, 0.89373561, 0.99074625, 0.97296204,\n       0.99316537, 0.90215394, 0.97495123, 0.99266625, 0.97259637,\n       0.97992884, 0.9834917 , 0.96654812, 0.97981995, 0.9770886 ,\n       0.99101768, 0.8943853 , 0.94494997, 0.99224194, 0.99072203,\n       0.96665617, 0.97937474, 0.94438829, 0.92853256, 0.96742508,\n       0.95521472, 0.97704436, 0.90319186, 0.99277663, 0.99302645,\n       0.98360882, 0.97361677, 0.94924144, 0.98835753, 0.9829464 ,\n       0.92328572, 0.99127484, 0.98865549, 0.9922278 , 0.98488978,\n       0.97896371, 0.96284268, 0.99182273, 0.99003037, 0.89432076,\n       0.88885524, 0.95808891, 0.98824509, 0.99304854, 0.92065286,\n       0.97773869, 0.95861903, 0.9926796 , 0.9892958 , 0.98917471,\n       0.96789399, 0.96247348, 0.94375881, 0.8974426 , 0.98999056])",
    "crumbs": [
      "Lecture notes",
      "Neural Network and DL",
      "Neural Network Basics",
      "Logistic Regression"
    ]
  }
]