{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Back Propagation in Linear Layer of Neural Network\"\n",
    "date: \"26-Sept-2024\"\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd.functional import jacobian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation through Linear Layer of a Neural Network using Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will follow the teachings of Justing Johnson in his [CS231n course](https://cs231n.stanford.edu/handouts/linear-backprop.pdf). We will derive the backpropagation for a linear layer. In this linear layer we will assume a batch of $N$ records, with $D$ features. Linear layer transforms a $D$ dimension input to a $M$ dimension output. The weights matrix is $W$ of size $D \\times M$ and the bias vector is $b$ of size $M$. The input to the layer is $X$ of size $N \\times D$ and the output is $Z$ of size $N \\times M$. The loss function is $L$.\n",
    "\n",
    "Mathematical notation:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "X &\\in \\mathbb{R}^{N \\times D} \\\\\n",
    "W &\\in \\mathbb{R}^{D \\times M} \\\\\n",
    "b &\\in \\mathbb{R}^{M} \\\\\n",
    "Z = XW + b &\\in \\mathbb{R}^{N \\times M} \\\\\n",
    "L &\\in \\mathbb{R}\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For the purpose of focusing on the backpopagation through linear layer only. We will also assume that $\\frac{\\partial L}{\\partial Z}$ is given to us. We will derive the backpropagation for $\\frac{\\partial L}{\\partial X}$, $\\frac{\\partial L}{\\partial W}$ and $\\frac{\\partial L}{\\partial b}$. Note that $\\frac{\\partial L}{\\partial Z}$ is of size $N \\times M$. as $Z$ is of size $N \\times M$. and L is a scalar.\n",
    "\n",
    "Components of $\\frac{\\partial L}{\\partial Z}$ are as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial Z} &= \\begin{bmatrix}\n",
    "\\frac{\\partial L}{\\partial Z_{11}} & \\frac{\\partial L}{\\partial Z_{12}} & \\cdots & \\frac{\\partial L}{\\partial Z_{1M}} \\\\\n",
    "\\frac{\\partial L}{\\partial Z_{21}} & \\frac{\\partial L}{\\partial Z_{22}} & \\cdots & \\frac{\\partial L}{\\partial Z_{2M}} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial L}{\\partial Z_{N1}} & \\frac{\\partial L}{\\partial Z_{N2}} & \\cdots & \\frac{\\partial L}{\\partial Z_{NM}}\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to use $\\frac{\\partial L}{\\partial Z}$ to calculate $\\frac{\\partial L}{\\partial X}$, $\\frac{\\partial L}{\\partial W}$ and $\\frac{\\partial L}{\\partial b}$. Since L is a scalar, $\\frac{\\partial L}{\\partial X}$, $\\frac{\\partial L}{\\partial W}$ and $\\frac{\\partial L}{\\partial b}$ will be of the same size as $X$ ($N \\times D$), $W$ ($D \\times M$) and $b$ ($M$) respectively.\n",
    "\n",
    "By the chain rule, we know that:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L}{\\partial X} &= \\frac{\\partial L}{\\partial Z}  \\frac{\\partial Z}{\\partial X} \\\\\n",
    "\\frac{\\partial L}{\\partial W} &= \\frac{\\partial L}{\\partial Z}  \\frac{\\partial Z}{\\partial W} \\\\\n",
    "\\frac{\\partial L}{\\partial b} &= \\frac{\\partial L}{\\partial Z}  \\frac{\\partial Z}{\\partial b}\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "::: {.callout-note}\n",
    "- $\\frac{\\partial Z}{\\partial X}$ is a Jacobian Tensor of size $(N \\times M), (N \\times D)$\n",
    "- $\\frac{\\partial Z}{\\partial W}$ is a Jacobian Tensor of size $(N \\times M), (D \\times M)$\n",
    "- $\\frac{\\partial Z}{\\partial b}$ is a Jacobian Tensor of size $M \\times N$.\n",
    ":::\n",
    "\n",
    "In a typical network you may see N (Batch size) = 64; D (feature dimension) = 4096; M (Output features) = 4096. That means $\\frac{\\partial Z}{\\partial X}$ will be a tensor of size $64 \\times 4096 \\times 64 \\times 4096$. Thats about 68 Billion numbers each using 32-bit float. Thats a lot of memory. Doing this operation in a naive way will be very slow and memory intensive. We will see how to do this in a more efficient way.\n",
    "\n",
    "However for most common neural network layers,  we can derive the compute the product $\\frac{\\partial L}{\\partial Z}  \\frac{\\partial Z}{\\partial X}$ without explicitly using the jacobian $\\frac{\\partial Z}{\\partial X}$. We will see how to do this in the next section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Deriving $\\frac{\\partial L}{\\partial X}$\n",
    "---\n",
    "\n",
    "We will start by deriving $\\frac{\\partial L}{\\partial X}$. Note Since L is scalar $\\frac{\\partial L}{\\partial X}$ has the same size as $X$ ($N \\times D$).\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "X_{11} & X_{12} & \\cdots & X_{1D} \\\\\n",
    "X_{21} & X_{22} & \\cdots & X_{2D} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "X_{N1} & X_{N2} & \\cdots & X_{ND}\n",
    "\\end{bmatrix}\n",
    "\\implies \n",
    "\\frac{\\partial L}{\\partial X} = \\begin{bmatrix}\n",
    "\\frac{\\partial L}{\\partial X_{11}} & \\frac{\\partial L}{\\partial X_{12}} & \\cdots & \\frac{\\partial L}{\\partial X_{1D}} \\\\\n",
    "\\frac{\\partial L}{\\partial X_{21}} & \\frac{\\partial L}{\\partial X_{22}} & \\cdots & \\frac{\\partial L}{\\partial X_{2D}} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial L}{\\partial X_{N1}} & \\frac{\\partial L}{\\partial X_{N2}} & \\cdots & \\frac{\\partial L}{\\partial X_{ND}}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Lets start by calculating $\\frac{\\partial L}{\\partial X_{mn}}$ for a single element $X_{mn}$ of $X$. We will use the chain rule to calculate this. We will calculate the derivative of $L$ with respect to $X_{mn}$ Also note $\\frac{\\partial L}{\\partial X_{mn}}$  is a scalar.\n",
    "\n",
    "By chain rule\n",
    "$$ \n",
    "\\frac{\\partial L}{\\partial X_{mn}}  = \\frac{\\partial L}{\\partial Z} \\frac{\\partial Z}{\\partial X_{mn}}\n",
    "$$\n",
    "NB: \n",
    "- $\\frac{\\partial L}{\\partial Z}$ is of size $N \\times M$.\n",
    "- $\\frac{\\partial Z}{\\partial X_{mn}}$ is of size ($N \\times M) \\times 1$.\n",
    "\n",
    "\n",
    "Now note \n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "Z &= XW + b \\\\\n",
    "Z_{ij} &= \\sum_{k=1}^{D} x_{ik} w_{kj} + b_j \\\\\n",
    "\\implies \\\\\n",
    "\\frac{\\partial Z_{ij}}{\\partial x_{mn}} &= w_{nj} \\text{ if } i = m \\text{ else } 0 \\\\\n",
    "\\frac{\\partial Z_{ij}}{\\partial w_{mn}} &= x_{im} \\text{ if } j = n \\text{ else } 0 \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "\n",
    "<!-- $$\n",
    "\\begin{align*}\n",
    "Z = XW + b\\\\\n",
    "Z_{ij} = \\sum_{k=1}^{D} x_{ik} w_{kj} + b_j \\\\\n",
    "\\implies \\\\\n",
    "\\frac{\\partial Z_{ij}}{\\partial x_{mn}} = w_{nj} \\text{ if } i = m \\text{ else } 0 \\\\\n",
    "\\frac{\\partial Z_{ij}}{\\partial w_{mn}} = x_{im} \\text{ if } j = n \\text{ else } 0 \\\\\n",
    "\\end{align*}\n",
    "$$ -->\n",
    "\n",
    "This $\\implies$\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L}{\\partial X_{mn}} &= \\sum_{i=1}^{N} \\sum_{j=1}^{M} \\frac{\\partial L}{\\partial Z_{ij}} \\frac{\\partial Z_{ij}}{\\partial X_{mn}} \\\\\n",
    "&= \\sum_{i=1}^{N} \\sum_{j=1}^{M} \\frac{\\partial L}{\\partial Z_{ij}} \\frac{\\partial Z_{ij}}{\\partial x_{mn}} \\\\\n",
    "&= \\sum_{i=1}^{N} \\sum_{j=1}^{M} \\frac{\\partial L}{\\partial Z_{ij}} w_{nj} \\text{ if } i = m \\text{ else } 0 \\\\\n",
    "&= \\sum_{j=1}^{M} \\frac{\\partial L}{\\partial Z_{mj}} w_{nj} \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "\n",
    "Notice this nothing but a dot product of the $m$ th row of $\\frac{\\partial L}{\\partial Z}$ and the $n$ th row of $W$  i.e ($n$ th column of $W^T$).\n",
    "\n",
    "::: {.callout-note}\n",
    " So given $Z = X @ W + b$ and $\\frac{\\partial L}{\\partial Z}$ we can calculate $\\frac{\\partial L}{\\partial X}$ as follows:\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L}{\\partial X} &= \\frac{\\partial L}{\\partial Z} W^T\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "$$\n",
    ":::\n",
    "\n",
    "Similarly we will now derive $\\frac{\\partial L}{\\partial W}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deriving $\\frac{\\partial L}{\\partial W}$\n",
    "---\n",
    "\n",
    "We will start by deriving $\\frac{\\partial L}{\\partial W}$. Note $\\frac{\\partial L}{\\partial W}$ has the same size as $W$ ($D \\times M$).\n",
    "\n",
    "\n",
    "$$\n",
    "W = \\begin{bmatrix}\n",
    "W_{11} & W_{12} & \\cdots & W_{1M} \\\\\n",
    "W_{21} & W_{22} & \\cdots & W_{2M} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "W_{D1} & W_{D2} & \\cdots & W_{DM}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$\\implies$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W} = \\begin{bmatrix}\n",
    "\\frac{\\partial L}{\\partial W_{11}} & \\frac{\\partial L}{\\partial W_{12}} & \\cdots & \\frac{\\partial L}{\\partial W_{1M}} \\\\\n",
    "\\frac{\\partial L}{\\partial W_{21}} & \\frac{\\partial L}{\\partial W_{22}} & \\cdots & \\frac{\\partial L}{\\partial W_{2M}} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial L}{\\partial W_{D1}} & \\frac{\\partial L}{\\partial W_{D2}} & \\cdots & \\frac{\\partial L}{\\partial W_{DM}}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Lets start by calculating $\\frac{\\partial L}{\\partial W_{mn}}$ for a single element $W_{mn}$ of $W$. We will use the chain rule to calculate this. We will calculate the derivative of $L$ with respect to $W_{mn}$ Also note $\\frac{\\partial L}{\\partial W_{mn}}$  is a scalar.\n",
    "\n",
    "By chain rule\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L}{\\partial W_{mn}} &= \\frac{\\partial L}{\\partial Z} \\frac{\\partial Z}{\\partial W_{mn}}\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "Now note\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "Z &= XW + b \\\\\n",
    "Z_{ij} &= \\sum_{k=1}^{D} x_{ik} w_{kj} + b_j \\\\\n",
    "\\implies &\\\\\n",
    "\\frac{\\partial Z_{ij}}{\\partial w_{mn}} &= x_{im} \\text{ if } j = n \\text{ else } 0 \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "This $\\implies$\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L}{\\partial W_{mn}} &= \\sum_{i=1}^{N} \\sum_{j=1}^{M} \\frac{\\partial L}{\\partial Z_{ij}} \\frac{\\partial Z_{ij}}{\\partial W_{mn}} \\\\\n",
    "&= \\sum_{i=1}^{N} \\sum_{j=1}^{M} \\frac{\\partial L}{\\partial Z_{ij}} \\frac{\\partial Z_{ij}}{\\partial w_{mn}} \\\\\n",
    "&= \\sum_{i=1}^{N} \\frac{\\partial L}{\\partial Z_{in}} x_{im} \\\\\n",
    "&= \\sum_{i=1}^{N} x_{im} \\frac{\\partial L}{\\partial Z_{in}} \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "Observe that this is nothing by dot product of m th column of $X$ ( or the m th row of $X^T$) and the n th column of $\\frac{\\partial L}{\\partial Z}$.\n",
    "\n",
    "\n",
    "::: {.callout-note}\n",
    " So given $Z = X @ W + b$ and $\\frac{\\partial L}{\\partial Z}$ we can calculate $\\frac{\\partial L}{\\partial W}$ as follows:\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L}{\\partial W} &= X^T \\frac{\\partial L}{\\partial Z}\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "$$\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deriving $\\frac{\\partial L}{\\partial b}$\n",
    "\n",
    "We will derive $\\frac{\\partial L}{\\partial b}$. Note $\\frac{\\partial L}{\\partial b}$ has the same size as $b$ ($M$).\n",
    "$$\n",
    "b = \\begin{bmatrix}\n",
    "b_{1} \\\\\n",
    "b_{2} \\\\\n",
    "\\vdots \\\\\n",
    "b_{M}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$\\implies$\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = \\begin{bmatrix}\n",
    "\\frac{\\partial L}{\\partial b_{1}} \\\\\n",
    "\\frac{\\partial L}{\\partial b_{2}} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial L}{\\partial b_{M}}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Lets start by calculating $\\frac{\\partial L}{\\partial b_{m}}$ for a single element $b_{m}$ of $b$. We will use the chain rule to calculate this. We will calculate the derivative of $L$ with respect to $b_{m}$ Also note $\\frac{\\partial L}{\\partial b_{m}}$  is a scalar.\n",
    "\n",
    "By chain rule\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L}{\\partial b_{m}} &= \\frac{\\partial L}{\\partial Z} \\frac{\\partial Z}{\\partial b_{m}}\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "Now note\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "Z &= XW + b \\\\\n",
    "Z_{ij} &= \\sum_{k=1}^{D} x_{ik} w_{kj} + b_j \\\\\n",
    "\\implies &\\\\\n",
    "\\frac{\\partial Z_{ij}}{\\partial b_{m}} &= 1 \\text{ if } j = m \\text{ else } 0 \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "This $\\implies$\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L}{\\partial b_{m}} &= \\sum_{i=1}^{N} \\sum_{j=1}^{M} \\frac{\\partial L}{\\partial Z_{ij}} \\frac{\\partial Z_{ij}}{\\partial b_{m}} \\\\\n",
    "&= \\sum_{i=1}^{N} \\sum_{j=1}^{M} \\frac{\\partial L}{\\partial Z_{ij}} \\frac{\\partial Z_{ij}}{\\partial b_{m}} \\\\\n",
    "&= \\sum_{i=1}^{N} \\frac{\\partial L}{\\partial Z_{im}} \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "Observe that this is nothing but the sum of the m th column of $\\frac{\\partial L}{\\partial Z}$.\n",
    "\n",
    "::: {.callout-note}\n",
    " So given $Z = X @ W + b$ and $\\frac{\\partial L}{\\partial Z}$ we can calculate $\\frac{\\partial L}{\\partial b}$ as follows:\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L}{\\partial b} &= \\sum_{i=1}^{N} \\frac{\\partial L}{\\partial Z_{i}}\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "$$\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will go back to pytorch and verify our derivations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Shape: torch.Size([5, 2])\n",
      "torch.Size([5, 2, 5, 3]) torch.Size([5, 2, 3, 2]) torch.Size([5, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "## a quick setup in pytorch\n",
    "N = 5; D = 3; M = 2\n",
    "X = torch.randn(N, D, requires_grad=False) ## disabling requires grad as we will do it all manually\n",
    "W = torch.randn(D, M, requires_grad=False)\n",
    "b = torch.randn(M, requires_grad=False)\n",
    "#Z = X @ W + b\n",
    "def linear_transform(X, W, b):\n",
    "    res = X @ W + b\n",
    "    print(f\"Output Shape: {res.shape}\")\n",
    "    return res\n",
    "\n",
    "dL_dZ = torch.randn(N, M, requires_grad=False)\n",
    "\n",
    "## not we are interested in dL_dW and dL_db and dL_dX\n",
    "\n",
    "jacobian_output = jacobian(linear_transform, (X, W, b))\n",
    "jacob_dZ_dX, jacob_dZ_dW, jacob_dZ_db = jacobian_output\n",
    "print(jacob_dZ_dX.shape, jacob_dZ_dW.shape, jacob_dZ_db.shape)\n",
    "\n",
    "#dL_dZ @ jacob_dZ_dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0938,  1.3802],\n",
       "         [-3.0765,  3.4443],\n",
       "         [ 1.4643,  0.1143]]),\n",
       " tensor([[-0.2672,  0.1566, -0.2825],\n",
       "         [-0.1333,  0.6751, -0.9973],\n",
       "         [-1.0372,  1.0880, -1.7853],\n",
       "         [-0.5966,  0.4912, -0.8339],\n",
       "         [ 0.5652, -0.4775,  0.8074]]),\n",
       " tensor([ 2.8203, -2.7928]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Now \n",
    "\n",
    "dl_dx = X.T @ dL_dZ \n",
    "dl_dw = dL_dZ @ W.T\n",
    "dl_db = dL_dZ.sum(dim=0)\n",
    "dl_dx, dl_dw, dl_db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual gradient dL/dX: \n",
      " tensor([[-0.2672,  0.1566, -0.2825],\n",
      "        [-0.1333,  0.6751, -0.9973],\n",
      "        [-1.0372,  1.0880, -1.7853],\n",
      "        [-0.5966,  0.4912, -0.8339],\n",
      "        [ 0.5652, -0.4775,  0.8074]])\n",
      "Jacobian-based dL/dX: \n",
      " tensor([[-0.2672,  0.1566, -0.2825],\n",
      "        [-0.1333,  0.6751, -0.9973],\n",
      "        [-1.0372,  1.0880, -1.7853],\n",
      "        [-0.5966,  0.4912, -0.8339],\n",
      "        [ 0.5652, -0.4775,  0.8074]])\n",
      "Manual gradient dL/dW: \n",
      " tensor([[ 0.0938,  1.3802],\n",
      "        [-3.0765,  3.4443],\n",
      "        [ 1.4643,  0.1143]])\n",
      "Jacobian-based dL/dW: \n",
      " tensor([[ 0.0938,  1.3802],\n",
      "        [-3.0765,  3.4443],\n",
      "        [ 1.4643,  0.1143]])\n",
      "Manual gradient dL/db: \n",
      " tensor([ 2.8203, -2.7928])\n",
      "Jacobian-based dL/db: \n",
      " tensor([ 2.8203, -2.7928])\n",
      "dL/dX close:  True\n",
      "dL/dW close:  True\n",
      "dL/db close:  True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd.functional import jacobian\n",
    "\n",
    "\n",
    "# Derived gradients using the formulas\n",
    "dl_dw = X.T @ dL_dZ\n",
    "dl_db = dL_dZ.sum(dim=0)\n",
    "dl_dx = dL_dZ @ W.T\n",
    "\n",
    "# Using the Jacobian to compute the gradients\n",
    "dl_dx_jacobian = torch.einsum('ij,ijkl->kl', dL_dZ, jacob_dZ_dX)\n",
    "dl_dw_jacobian = torch.einsum('ij,ijkl->kl', dL_dZ, jacob_dZ_dW)\n",
    "dl_db_jacobian = torch.einsum('ij,ijk->k', dL_dZ, jacob_dZ_db)\n",
    "\n",
    "# Compare the results\n",
    "print(\"Manual gradient dL/dX: \\n\", dl_dx)\n",
    "print(\"Jacobian-based dL/dX: \\n\", dl_dx_jacobian)\n",
    "print(\"Manual gradient dL/dW: \\n\", dl_dw)\n",
    "print(\"Jacobian-based dL/dW: \\n\", dl_dw_jacobian)\n",
    "print(\"Manual gradient dL/db: \\n\", dl_db)\n",
    "print(\"Jacobian-based dL/db: \\n\", dl_db_jacobian)\n",
    "\n",
    "# Check if they are equal\n",
    "print(\"dL/dX close: \", torch.allclose(dl_dx, dl_dx_jacobian))\n",
    "print(\"dL/dW close: \", torch.allclose(dl_dw, dl_dw_jacobian))\n",
    "print(\"dL/db close: \", torch.allclose(dl_db, dl_db_jacobian))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A quick note on jacobian product for gradient backpropagation for multivariate functions\n",
    "---\n",
    "Let $F$ : from (m,n) to (p,q) be a function.  Let $G$ be a function from (p,q) to (r,s). We want to see the composition of $F$ and $G$. So the composition function $H = G \\circ F$. H is a map from $(m,n) \\to (x,y)$. Let $J_1$ be the jacobian of $F$ with respect to $X$. Let $J_2$ be the jacobian of $G$ with respect to $Y$. Then the jacobian of $H$ with respect to $X$ is given by:\n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "H(X) &= G(F(X)) \\\\\n",
    "\\frac{\\partial H}{\\partial X} &= \\frac{\\partial G}{\\partial F} \\frac{\\partial F}{\\partial X} \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "Where $J_1 = \\frac{\\partial F}{\\partial X}$ and $J_2 = \\frac{\\partial G}{\\partial Y}$. \n",
    "\n",
    "Note the dimensions of the jacobian matrices. $J_1$ is of size $(m \\times n) \\times (p \\times q)$ and $J_2$ is of size $(p \\times q) \\times (r \\times s)$. The jacobian of $H$ with respect to $X$ is of size $(m \\times n) \\times (r \\times s)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jacob_F: torch.Size([5, 3, 5, 4])\n",
      "jacob_G: torch.Size([5, 2, 5, 3])\n",
      "jacob_H: torch.Size([5, 2, 5, 4])\n",
      "torch.Size([5, 2, 5, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## lets do it with autograd now\n",
    "m,n = 5, 4\n",
    "p,q = 4, 3\n",
    "r, s = 3, 2\n",
    "\n",
    "x = torch.randn(m, n, requires_grad=False)\n",
    "W_1 = torch.randn(p, q, requires_grad=False)\n",
    "W_2 = torch.randn(r, s, requires_grad=False)\n",
    "\n",
    "def linear_transform(x, W):\n",
    "    return x @ W\n",
    "\n",
    "def linear_transform_G(x, W1, W2):\n",
    "    out_1 = linear_transform(x, W1)\n",
    "    return linear_transform(out_1, W2)\n",
    "\n",
    "\n",
    "\n",
    "out_1 = linear_transform(x, W_1)\n",
    "out_2 = linear_transform(out_1, W_2)\n",
    "\n",
    "jacobian_F = jacobian(linear_transform, (x, W_1))\n",
    "jacobian_G = jacobian(linear_transform, (out_1, W_2))\n",
    "jacobian_H = jacobian(linear_transform_G, (x, W_1, W_2))\n",
    "\n",
    "## lets print out the shapes\n",
    "print(f\"jacob_F: {jacobian_F[0].shape}\")\n",
    "print(f\"jacob_G: {jacobian_G[0].shape}\")\n",
    "print(f\"jacob_H: {jacobian_H[0].shape}\")\n",
    "\n",
    "jacob_H_manual = torch.einsum('abij,ijxy->abxy', jacobian_G[0], jacobian_F[0])\n",
    "print(jacob_H_manual.shape)\n",
    "torch.allclose(jacobian_H[0], jacob_H_manual)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
