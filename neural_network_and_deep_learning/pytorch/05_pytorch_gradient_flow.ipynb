{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradinent FLow\n",
    "\n",
    "In this notebook lets look at gradient flow through different functions, scalar fields and vector fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd.functional import jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_sum(x):\n",
    "    return torch.sum(x)\n",
    "\n",
    "def my_mean(x):\n",
    "    return torch.mean(x)\n",
    "\n",
    "def my_exp(x):\n",
    "    return torch.exp(x)\n",
    "\n",
    "\n",
    "def my_log(x):\n",
    "    return torch.log(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets look at gradient of sum function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0, 4.0], requires_grad=True)\n",
    "y = my_sum(x)\n",
    "y.backward()\n",
    "print(x.grad)\n",
    "\n",
    "## reset the gradient\n",
    "x.grad.zero_()\n",
    "print(jacobian(my_sum, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets look at gradient of mean function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2500, 0.2500, 0.2500, 0.2500])\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500])\n"
     ]
    }
   ],
   "source": [
    "y  = my_mean(x)\n",
    "y.backward()\n",
    "print(x.grad)\n",
    "\n",
    "## reset the gradient\n",
    "x.grad.zero_()\n",
    "print(jacobian(my_mean, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets look at gradient of exp function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.7183,  7.3891, 20.0855, 54.5982])\n",
      "tensor([[ 2.7183,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  7.3891,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000, 20.0855,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000, 54.5982]])\n"
     ]
    }
   ],
   "source": [
    "y = my_exp(x)\n",
    "y.backward(torch.ones_like(x))\n",
    "print(x.grad)\n",
    "\n",
    "## reset the gradient\n",
    "x.grad.zero_()\n",
    "print(jacobian(my_exp, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "W = torch.tensor([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]], requires_grad=True)\n",
    "b = torch.tensor([0.1, 0.2], requires_grad=True)\n",
    "\n",
    "# Forward pass\n",
    "def my_affine(x, W, b):\n",
    "    return torch.matmul(W, x) + b\n",
    "\n",
    "affine_jacobian = jacobian(my_affine, (x, W, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1000, 0.2000, 0.3000],\n",
       "         [0.4000, 0.5000, 0.6000]]),\n",
       " tensor([[[1., 2., 3.],\n",
       "          [0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.],\n",
       "          [1., 2., 3.]]]),\n",
       " tensor([[1., 0.],\n",
       "         [0., 1.]]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "affine_jacobian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets look at gradient of log function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 0.5000, 0.3333, 0.2500])\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.3333, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.2500]])\n"
     ]
    }
   ],
   "source": [
    "y = my_log(x)\n",
    "y.backward(torch.ones_like(x))\n",
    "print(x.grad)\n",
    "\n",
    "## reset the gradient\n",
    "x.grad.zero_()\n",
    "print(jacobian(my_log, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W.grad (dL/dW): tensor([[1., 2., 3.],\n",
      "        [1., 2., 3.]])\n",
      "dL/dz: tensor([1., 1.])\n",
      "dz/dW: tensor([[1., 2., 3.],\n",
      "        [1., 2., 3.]])\n",
      "z.grad: tensor([3., 3.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Initialize tensors\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "W = torch.tensor([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]], requires_grad=True)\n",
    "b = torch.tensor([0.1, 0.2], requires_grad=True)\n",
    "\n",
    "# Forward pass\n",
    "z = W @ x + b\n",
    "z.retain_grad()\n",
    "\n",
    "# Compute L\n",
    "L = z.sum()\n",
    "\n",
    "# Clear previous gradients\n",
    "W.grad = None\n",
    "b.grad = None\n",
    "x.grad = None\n",
    "\n",
    "# Backward pass to get dL/dW (global gradient)\n",
    "L.backward(retain_graph=True)\n",
    "\n",
    "# Check W.grad (dL/dW)\n",
    "print(\"W.grad (dL/dW):\", W.grad)\n",
    "\n",
    "# Compute local gradients\n",
    "with torch.no_grad():\n",
    "    # dL/dz\n",
    "    grad_L_z = torch.autograd.grad(L, z, retain_graph=True)[0]\n",
    "    print(\"dL/dz:\", grad_L_z)\n",
    "\n",
    "    # dz/dW\n",
    "    grad_z_W = torch.autograd.grad(z, W, grad_outputs=torch.ones_like(z), retain_graph=True)[0]\n",
    "    print(\"dz/dW:\", grad_z_W)\n",
    "\n",
    "# Checking z.grad\n",
    "print(\"z.grad:\", z.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 3.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W.grad (dL/dW): tensor([[1., 2., 3.],\n",
      "        [1., 2., 3.]])\n",
      "dL/dz: tensor([1., 1.])\n",
      "dz/dW: tensor([[1., 2., 3.],\n",
      "        [1., 2., 3.]])\n",
      "z.grad: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2z/hm_pwbd91vjf6y817f4mp5pc0000gn/T/ipykernel_36471/2424213833.py:30: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  print(\"z.grad:\", z.grad)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Initialize tensors\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "W = torch.tensor([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]], requires_grad=True)\n",
    "b = torch.tensor([0.1, 0.2], requires_grad=True)\n",
    "\n",
    "# Forward pass\n",
    "z = W @ x + b\n",
    "\n",
    "# Compute L\n",
    "L = z.sum()\n",
    "\n",
    "# Backward pass to get dL/dW (global gradient)\n",
    "L.backward(retain_graph=True)\n",
    "\n",
    "# Check W.grad (dL/dW)\n",
    "print(\"W.grad (dL/dW):\", W.grad)\n",
    "\n",
    "# Compute local gradients\n",
    "# dL/dz\n",
    "grad_L_z = torch.autograd.grad(L, z, retain_graph=True)[0]\n",
    "print(\"dL/dz:\", grad_L_z)\n",
    "\n",
    "# dz/dW\n",
    "grad_z_W = torch.autograd.grad(z, W, grad_outputs=torch.ones_like(z), retain_graph=True)[0]\n",
    "print(\"dz/dW:\", grad_z_W)\n",
    "\n",
    "# Checking z.grad\n",
    "print(\"z.grad:\", z.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W.grad (dL/dW): tensor([[1., 2., 3.],\n",
      "        [1., 2., 3.]])\n",
      "z.grad: tensor([1., 1.])\n",
      "dL/dz: tensor([1., 1.])\n",
      "dz/dW: tensor([[1., 2., 3.],\n",
      "        [1., 2., 3.]])\n",
      "dL/dW: tensor([2., 4., 6.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Initialize tensors\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "W = torch.tensor([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]], requires_grad=True)\n",
    "b = torch.tensor([0.1, 0.2], requires_grad=True)\n",
    "\n",
    "# Forward pass\n",
    "z = W @ x + b\n",
    "\n",
    "# Retain gradient for z, which is not a leaf tensor\n",
    "z.retain_grad()\n",
    "\n",
    "# Compute L\n",
    "L = z.sum()\n",
    "\n",
    "# Backward pass to get dL/dW (global gradient)\n",
    "L.backward(retain_graph=True)\n",
    "\n",
    "# Check W.grad (dL/dW)\n",
    "print(\"W.grad (dL/dW):\", W.grad)\n",
    "\n",
    "# Check z.grad after backward pass\n",
    "print(\"z.grad:\", z.grad)\n",
    "\n",
    "# Compute local gradients manually\n",
    "# dL/dz\n",
    "grad_L_z = torch.autograd.grad(L, z, retain_graph=True)[0]\n",
    "print(\"dL/dz:\", grad_L_z)\n",
    "\n",
    "# dz/dW\n",
    "grad_z_W = torch.autograd.grad(z, W, grad_outputs=torch.ones_like(z), retain_graph=True)[0]\n",
    "print(\"dz/dW:\", grad_z_W)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Compute dL/dW\n",
    "    grad_L_W = grad_L_z @ grad_z_W\n",
    "    print(\"dL/dW:\", grad_L_W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W.grad (dL/dW): tensor([[1., 2., 3.],\n",
      "        [1., 2., 3.]])\n",
      "z.grad: tensor([1., 1.])\n",
      "dL/dz: tensor([1., 1.])\n",
      "dz/dW: tensor([[1., 2., 3.],\n",
      "        [1., 2., 3.]])\n",
      "Manual dL/dW: tensor([[1., 2., 3.],\n",
      "        [1., 2., 3.]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Initialize tensors\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "W = torch.tensor([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]], requires_grad=True)\n",
    "b = torch.tensor([0.1, 0.2], requires_grad=True)\n",
    "\n",
    "# Forward pass\n",
    "z = W @ x + b\n",
    "\n",
    "# Retain gradient for z, which is not a leaf tensor\n",
    "z.retain_grad()\n",
    "\n",
    "# Compute L\n",
    "L = z.sum()\n",
    "\n",
    "# Backward pass to get dL/dW (global gradient)\n",
    "L.backward(retain_graph=True)\n",
    "\n",
    "# Check W.grad (dL/dW)\n",
    "print(\"W.grad (dL/dW):\", W.grad)\n",
    "\n",
    "# Check z.grad after backward pass\n",
    "print(\"z.grad:\", z.grad)\n",
    "\n",
    "# Compute local gradients manually\n",
    "# dL/dz\n",
    "grad_L_z = torch.autograd.grad(L, z, retain_graph=True)[0]\n",
    "print(\"dL/dz:\", grad_L_z)\n",
    "\n",
    "# dz/dW\n",
    "grad_z_W = torch.autograd.grad(z, W, grad_outputs=torch.ones_like(z), retain_graph=True)[0]\n",
    "print(\"dz/dW:\", grad_z_W)\n",
    "\n",
    "# Compute dL/dW correctly\n",
    "# Since dz/dW is a tensor of shape (2, 3) (like W), we need to properly sum the contributions.\n",
    "# dL/dW = grad_L_z (2, 1) * dz/dW (2, 3)\n",
    "# We should use matrix multiplication and broadcasting\n",
    "\n",
    "grad_L_W_manual = torch.outer(grad_L_z, x)  # This gives you the correct (2, 3) shape\n",
    "print(\"Manual dL/dW:\", grad_L_W_manual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Tensor' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmatmul(W, x) \u001b[38;5;241m+\u001b[39m b\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 6\u001b[0m     affine_jacobian \u001b[38;5;241m=\u001b[39m \u001b[43mjacobian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_affine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m jacob_dz_dx, jacbo_dz_dW, jacob_dz_db \u001b[38;5;241m=\u001b[39m affine_jacobian\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Tensor' object is not callable"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "def my_affine(x, W, b):\n",
    "    return torch.matmul(W, x) + b\n",
    "\n",
    "with torch.no_grad():\n",
    "    affine_jacobian = jacobian(my_affine, (x, W, b))\n",
    "\n",
    "jacob_dz_dx, jacbo_dz_dW, jacob_dz_db = affine_jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 2., 3.],\n",
       "         [0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.],\n",
       "         [1., 2., 3.]]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jacbo_dz_dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [1., 2., 3.]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_z_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1.]),\n",
       " tensor([[[1., 2., 3.],\n",
       "          [0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.],\n",
       "          [1., 2., 3.]]]))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_L_z, jacbo_dz_dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [1., 2., 3.]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_L_z @ jacbo_dz_dW "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_L_z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jacbo_dz_dW.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([42.1000, 87.7000, 88.2000], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([10.0, 15.0, 20.0, 30.0, 40.0], requires_grad=True)\n",
    "W = torch.tensor([[0.1, 0.2, 0.3, 0.4, 0.5], [0.4, 0.5, 0.6, 0.8, 1.0],  [0.4, 0.5, 0.6, 0.8, 1.0]], requires_grad=True)\n",
    "b = torch.tensor([0.1, 0.2, 0.7], requires_grad=True)\n",
    "y_out = torch.tensor([1.0, 5.0, 10.0], requires_grad=False)\n",
    "\n",
    "def my_affine(x, W, b):\n",
    "    return W @ x + b\n",
    "\n",
    "def my_affine_sum(x, W, b):\n",
    "    y_out = torch.tensor([1.0, 5.0, 10.0], requires_grad=False)\n",
    "    affine_out = my_affine(x, W, b)\n",
    "    print(affine_out)\n",
    "    return F.mse_loss(affine_out, y_out)\n",
    "\n",
    "affine_jacobian = jacobian(my_affine, (x, W, b))\n",
    "affine_sum_jacobian = jacobian(my_affine_sum, (x, W, b))\n",
    "\n",
    "## now we will compute via backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 45.6467,  59.1133,  72.5800,  96.7733, 120.9667]),\n",
       " tensor([[ 274.0000,  411.0000,  548.0000,  822.0000, 1096.0000],\n",
       "         [ 551.3333,  827.0000, 1102.6666, 1654.0000, 2205.3333],\n",
       "         [ 521.3333,  782.0000, 1042.6666, 1564.0000, 2085.3333]]),\n",
       " tensor([27.4000, 55.1333, 52.1333]))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "affine_sum_jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-27.4000, -55.1333, -52.1333])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([27.4000, 55.1333, 52.1333]), tensor([-27.4000, -55.1333, -52.1333]))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_1 = torch.tensor([42.1000, 87.7000, 88.2000], requires_grad=False)\n",
    "input_2 = torch.tensor([1.0, 5.0, 10.0], requires_grad=False)\n",
    "\n",
    "print(2*(input_2 - input_1)/3)\n",
    "\n",
    "jacobian(F.mse_loss,(input_1, input_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W.grad tensor([[10., 15., 20.],\n",
      "        [10., 15., 20.]])\n",
      "z.grad: tensor([1., 1.])\n",
      "dL/dz: tensor([1., 1.])\n"
     ]
    }
   ],
   "source": [
    "## now lets compute the jacobian via backward pass\n",
    "z = my_affine(x, W, b)\n",
    "z.retain_grad()\n",
    "\n",
    "L = z.sum()\n",
    "L.backward(retain_graph=True)\n",
    "\n",
    "## check the global gradient\n",
    "print(\"W.grad\" ,W.grad)\n",
    "# Check z.grad after backward pass\n",
    "print(\"z.grad:\", z.grad)\n",
    "\n",
    "\n",
    "## \n",
    "grad_L_z = torch.autograd.grad(L, z, retain_graph=True)[0]\n",
    "print(\"dL/dz:\", grad_L_z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.5000, 0.7000, 0.9000]),\n",
       " tensor([[10., 15., 20.],\n",
       "         [10., 15., 20.]]),\n",
       " tensor([1., 1.]))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "affine_sum_jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1000, 0.2000, 0.3000],\n",
       "         [0.4000, 0.5000, 0.6000]]),\n",
       " tensor([[[10., 15., 20.],\n",
       "          [ 0.,  0.,  0.]],\n",
       " \n",
       "         [[ 0.,  0.,  0.],\n",
       "          [10., 15., 20.]]]),\n",
       " tensor([[1., 0.],\n",
       "         [0., 1.]]))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "affine_jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_out = torch.tensor([1.0, 5.0])\n",
    "y_out_1 = torch.tensor([2.0, 6.0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define functions\n",
    "# def z(W, x, b):\n",
    "#     return torch.matmul(W, x) + b\n",
    "\n",
    "# def a(z):\n",
    "#     return F.softmax(z, dim=1)\n",
    "\n",
    "# def L(a, truth):\n",
    "#     return F.mse_loss(a, truth)\n",
    "\n",
    "# # Create input tensors\n",
    "# W = torch.randn(2, 3, requires_grad=True)\n",
    "# x = torch.randn(3, 1, requires_grad=True)\n",
    "# b = torch.randn(2, 1, requires_grad=True)\n",
    "# truth = torch.randn(2, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch gradients:\n",
      "dL/dW (PyTorch): tensor([[ 0.0106,  0.0213],\n",
      "        [-0.0106, -0.0213]])\n",
      "dL/db (PyTorch): tensor([ 0.0106, -0.0106])\n",
      "dL/dx (PyTorch): tensor([ 0.0021, -0.0138])\n"
     ]
    }
   ],
   "source": [
    "# Define inputs\n",
    "x = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "W = torch.tensor([[0.5, -0.5], [0.3, 0.8]], requires_grad=True)\n",
    "b = torch.tensor([0.1, 0.2], requires_grad=True)\n",
    "\n",
    "# Ground truth\n",
    "truth = torch.tensor([0.0, 1.0])\n",
    "\n",
    "# Forward pass\n",
    "z = W @ x + b\n",
    "a = F.softmax(z, dim=0)\n",
    "L = F.mse_loss(a, truth)\n",
    "\n",
    "# Perform backward pass to get gradients from PyTorch\n",
    "L.backward()\n",
    "\n",
    "# Save the gradients before zeroing them out\n",
    "dL_dW_pytorch = W.grad.clone()\n",
    "dL_db_pytorch = b.grad.clone()\n",
    "dL_dx_pytorch = x.grad.clone()\n",
    "\n",
    "# Reset gradients to manually compute\n",
    "W.grad.zero_()\n",
    "b.grad.zero_()\n",
    "x.grad.zero_()\n",
    "\n",
    "print(\"PyTorch gradients:\")\n",
    "print(\"dL/dW (PyTorch):\", dL_dW_pytorch)\n",
    "print(\"dL/db (PyTorch):\", dL_db_pytorch)\n",
    "print(\"dL/dx (PyTorch):\", dL_dx_pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Manual gradients:\n",
      "dL/dW (Manual): tensor([[[ 0.0106,  0.0213],\n",
      "         [-0.0106, -0.0213]]], grad_fn=<CloneBackward0>)\n",
      "dL/db (Manual): tensor([ 0.0106, -0.0106], grad_fn=<SqueezeBackward4>)\n",
      "dL/dx (Manual): tensor([ 0.0106, -0.0053], grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "dL_da = 2 * (a - truth) / a.size(0)\n",
    "\n",
    "# Compute the gradient of a with respect to z (Jacobian)\n",
    "a_diag = torch.diag(a)\n",
    "J = a_diag - torch.outer(a, a)\n",
    "\n",
    "# Compute the gradient of z with respect to W, x, and b\n",
    "dz_dW = x.unsqueeze(0)  # W is applied to x: z = W @ x, so dz/dW = x^T\n",
    "dz_db = torch.eye(z.size(0))  # dz/db = I\n",
    "dz_dx = W.T  # W is applied to x: z = W @ x, so dz/dx = W^T\n",
    "\n",
    "# Chain rule: compute dL/dz\n",
    "dL_dz = dL_da @ J\n",
    "\n",
    "# Chain rule: compute dL/dW, dL/db, dL/dx\n",
    "dL_dW_manual = dL_dz.unsqueeze(1) @ dz_dW.unsqueeze(0)\n",
    "dL_db_manual = dL_dz @ dz_db\n",
    "dL_dx_manual = dL_dz @ dz_dx\n",
    "\n",
    "print(\"\\nManual gradients:\")\n",
    "print(\"dL/dW (Manual):\", dL_dW_manual)\n",
    "print(\"dL/db (Manual):\", dL_db_manual)\n",
    "print(\"dL/dx (Manual):\", dL_dx_manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison:\n",
      "dL/dW difference: True\n",
      "dL/db difference: True\n",
      "dL/dx difference: False\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nComparison:\")\n",
    "print(\"dL/dW difference:\", torch.allclose(dL_dW_manual, dL_dW_pytorch))\n",
    "print(\"dL/db difference:\", torch.allclose(dL_db_manual, dL_db_pytorch))\n",
    "print(\"dL/dx difference:\", torch.allclose(dL_dx_manual, dL_dx_pytorch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recomputed dL/dx (Manual): tensor([ 0.0106, -0.0053], grad_fn=<SqueezeBackward4>)\n",
      "dL/dx (PyTorch): tensor([ 0.0021, -0.0138])\n"
     ]
    }
   ],
   "source": [
    "# Recompute dL/dz using the correct softmax gradient and MSE loss derivative\n",
    "dL_dz = dL_da @ J\n",
    "\n",
    "# Compute dL/dx manually\n",
    "dL_dx_manual = dL_dz @ dz_dx\n",
    "\n",
    "print(\"Recomputed dL/dx (Manual):\", dL_dx_manual)\n",
    "print(\"dL/dx (PyTorch):\", dL_dx_pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recomputed dL/dx (Manual): tensor([ 0.0106, -0.0053], grad_fn=<SqueezeBackward4>)\n",
      "dL/dx (PyTorch): tensor([ 0.0021, -0.0138])\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Compute dL/da\n",
    "dL_da = 2 * (a - truth) / a.size(0)\n",
    "\n",
    "# Step 2: Compute Jacobian da/dz\n",
    "a_diag = torch.diag(a)\n",
    "J = a_diag - torch.outer(a, a)\n",
    "\n",
    "# Step 3: Compute dL/dz\n",
    "dL_dz = dL_da @ J\n",
    "\n",
    "# Step 4: Compute dz/dx\n",
    "dz_dx = W.T\n",
    "\n",
    "# Step 5: Manually compute dL/dx\n",
    "dL_dx_manual = dL_dz @ dz_dx\n",
    "\n",
    "print(\"Recomputed dL/dx (Manual):\", dL_dx_manual)\n",
    "print(\"dL/dx (PyTorch):\", dL_dx_pytorch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum difference in W.grad: tensor(0.)\n",
      "Maximum difference in b.grad: tensor(0.)\n",
      "Maximum difference in x.grad: tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Dimensions\n",
    "n = 5  # Input dimension\n",
    "m = 3  # Output dimension\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Initialize variables\n",
    "x = torch.randn(n, requires_grad=True)\n",
    "W = torch.randn(m, n, requires_grad=True)\n",
    "b = torch.randn(m, requires_grad=True)\n",
    "truth = torch.randn(m)\n",
    "\n",
    "# Forward pass\n",
    "z = W @ x + b  # Shape: (m,)\n",
    "a = F.softmax(z, dim=0)  # Shape: (m,)\n",
    "L = F.mse_loss(a, truth, reduction='mean')  # Scalar loss\n",
    "\n",
    "# Autograd computation\n",
    "L.backward()\n",
    "\n",
    "# Save gradients computed by autograd\n",
    "grad_W_autograd = W.grad.clone()\n",
    "grad_b_autograd = b.grad.clone()\n",
    "grad_x_autograd = x.grad.clone()\n",
    "\n",
    "# Zero gradients to prevent accumulation\n",
    "W.grad.zero_()\n",
    "b.grad.zero_()\n",
    "x.grad.zero_()\n",
    "\n",
    "# Manually compute gradients\n",
    "mse_factor = 1.0 / m\n",
    "\n",
    "# Step 1: Compute dL/da\n",
    "dL_da = 2 * mse_factor * (a.detach() - truth)  # Shape: (m,)\n",
    "\n",
    "# Step 2: Compute s = a^T * dL/da\n",
    "s = torch.dot(a.detach(), dL_da)  # Scalar\n",
    "\n",
    "# Step 3: Compute dL/dz\n",
    "dL_dz = a.detach() * (dL_da - s)  # Shape: (m,)\n",
    "\n",
    "# Step 4: Compute gradients w.r.t. W, b, x\n",
    "dL_dW_manual = dL_dz.unsqueeze(1) @ x.detach().unsqueeze(0)  # Shape: (m, n)\n",
    "dL_db_manual = dL_dz  # Shape: (m,)\n",
    "dL_dx_manual = W.detach().t() @ dL_dz  # Shape: (n,)\n",
    "\n",
    "# Compare gradients\n",
    "print(\"Maximum difference in W.grad:\", torch.max(torch.abs(grad_W_autograd - dL_dW_manual)))\n",
    "print(\"Maximum difference in b.grad:\", torch.max(torch.abs(grad_b_autograd - dL_db_manual)))\n",
    "print(\"Maximum difference in x.grad:\", torch.max(torch.abs(grad_x_autograd - dL_dx_manual)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradients w.r.t W:\n",
      "Autograd gradient:\n",
      " tensor([[-5.1207e-04,  9.7506e-05,  7.2401e-04, -1.8889e-04,  3.6038e-04],\n",
      "        [ 1.3595e-01, -2.5887e-02, -1.9222e-01,  5.0148e-02, -9.5679e-02],\n",
      "        [-1.3544e-01,  2.5789e-02,  1.9149e-01, -4.9959e-02,  9.5318e-02]])\n",
      "Manual gradient:\n",
      " tensor([[-5.1207e-04,  9.7506e-05,  7.2401e-04, -1.8889e-04,  3.6038e-04],\n",
      "        [ 1.3595e-01, -2.5887e-02, -1.9222e-01,  5.0148e-02, -9.5679e-02],\n",
      "        [-1.3544e-01,  2.5789e-02,  1.9149e-01, -4.9959e-02,  9.5318e-02]])\n",
      "Difference:\n",
      " tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "\n",
      "Gradients w.r.t b:\n",
      "Autograd gradient:\n",
      " tensor([-0.0003,  0.0882, -0.0879])\n",
      "Manual gradient:\n",
      " tensor([-0.0003,  0.0882, -0.0879])\n",
      "Difference:\n",
      " tensor([0., 0., 0.])\n",
      "\n",
      "Gradients w.r.t x:\n",
      "Autograd gradient:\n",
      " tensor([-0.0630,  0.0657, -0.1086,  0.1757,  0.0383])\n",
      "Manual gradient:\n",
      " tensor([-0.0630,  0.0657, -0.1086,  0.1757,  0.0383])\n",
      "Difference:\n",
      " tensor([0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# Compare gradients\n",
    "\n",
    "print(\"\\nGradients w.r.t W:\")\n",
    "print(\"Autograd gradient:\\n\", grad_W_autograd)\n",
    "print(\"Manual gradient:\\n\", dL_dW_manual)\n",
    "print(\"Difference:\\n\", grad_W_autograd - dL_dW_manual)\n",
    "\n",
    "print(\"\\nGradients w.r.t b:\")\n",
    "print(\"Autograd gradient:\\n\", grad_b_autograd)\n",
    "print(\"Manual gradient:\\n\", dL_db_manual)\n",
    "print(\"Difference:\\n\", grad_b_autograd - dL_db_manual)\n",
    "\n",
    "print(\"\\nGradients w.r.t x:\")\n",
    "print(\"Autograd gradient:\\n\", grad_x_autograd)\n",
    "print(\"Manual gradient:\\n\", dL_dx_manual)\n",
    "print(\"Difference:\\n\", grad_x_autograd - dL_dx_manual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
