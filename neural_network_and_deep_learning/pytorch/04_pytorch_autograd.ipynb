{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from ipywidgets import fixed  # Importing fixed\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multiple_functions(func_list,x_range_start=-10, x_range_end=10,  num_points=100, log_scale=False, **kwargs):\n",
    "    x = np.linspace(x_range_start, x_range_end, num_points)\n",
    "    for func in func_list:\n",
    "        y = func(x)\n",
    "        plt.plot(x, y, label=f'{func.__name__}', **kwargs)\n",
    "    \n",
    "    if log_scale:\n",
    "        plt.yscale('log')\n",
    "    \n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title('Plot of Multiple Functions')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets start with a simple scalar example\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b5ca2262b934dfc8de27a54ff52b0f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=-10.0, description='x_range_start', max=0.0, min=-20.0, step=0.5), Outâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_multiple_functions(func_list, x_range_start=-10, x_range_end=10, num_points=100, log_scale=False, **kwargs)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Widget to adjust x_range_start\n",
    "x_range_start_slider = widgets.FloatSlider(min=-20, max=0, step=0.5, value=-10, description='x_range_start')\n",
    "\n",
    "# Interactive function\n",
    "widgets.interact(plot_multiple_functions, \n",
    "                 func_list=fixed([sigmoid]), \n",
    "                 x_range_start=x_range_start_slider, \n",
    "                 x_range_end=fixed(10), \n",
    "                 num_points=fixed(100), \n",
    "                 log_scale=fixed(False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight Parameter containing:\n",
      "tensor([[ 0.4286,  0.3634,  0.3349, -0.0226,  0.3046],\n",
      "        [-0.1771, -0.4120, -0.4424,  0.0497, -0.0045],\n",
      "        [-0.1930, -0.1363, -0.0225,  0.2749, -0.0244]], requires_grad=True)\n",
      "bias Parameter containing:\n",
      "tensor([ 0.2026, -0.1183,  0.1449], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "my_test_linear_transform = torch.nn.Linear(5,3)\n",
    "for name, param in my_test_linear_transform.named_parameters():\n",
    "    print(name, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets create a sample input dataset. Input is 10 samples with 5 features. Output is 10 samples with 3 classes\n",
    "x = torch.randn(10,5)\n",
    "y_target = torch.randint(0,3,(10,))\n",
    "\n",
    "\n",
    "x.requires_grad = False\n",
    "weight_matrix = my_test_linear_transform.weight.detach().clone()\n",
    "weight_matrix.requires_grad = False\n",
    "bias = my_test_linear_transform.bias.detach().clone()\n",
    "bias.requires_grad = False\n",
    "\n",
    "## lets create a simple linear and compute loss\n",
    "with torch.no_grad():\n",
    "    y_a_manual = my_test_linear_transform(x)\n",
    "    y_a_manual_mm = x @ weight_matrix.t() + bias\n",
    "    y_a_softmax_manual = F.softmax(y_a_manual, dim=1)\n",
    "    loss_manual_vec = F.cross_entropy(y_a_manual, y_target, reduction='none')\n",
    "    loss_manual = loss_manual_vec.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example tensors\n",
    "# matrix_tensor = torch.randn(10, 5)  # A (10, 5) tensor\n",
    "# index_tensor = torch.randint(0, 5, (10,))  # An index tensor of shape (10) with values between 0 and 4\n",
    "\n",
    "# # Extract values\n",
    "# extracted_values = matrix_tensor[torch.arange(matrix_tensor.size(0)), index_tensor]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To derive the gradient of the softmax activation function, let's start with the definition of the softmax function. The softmax function is often used in multi-class classification tasks, and it is defined as follows:\n",
    "\n",
    "For a vector $\\mathbf{z} = [z_1, z_2, \\dots, z_n]$, the softmax function produces an output vector $\\mathbf{y} = [y_1, y_2, \\dots, y_n]$, where each component $y_i$ is given by:\n",
    "\n",
    "$$\n",
    "y_i = \\frac{e^{z_i}}{\\sum_{j=1}^{n} e^{z_j}}\n",
    "$$\n",
    "\n",
    "Here, $\\mathbf{y}$ represents the probabilities for each class, and each $y_i$ is constrained to lie between 0 and 1, with the sum of all $y_i$ equal to 1.\n",
    "\n",
    "### Deriving the Gradient\n",
    "\n",
    "The goal is to find the gradient of the softmax function with respect to its inputs $\\mathbf{z}$. Specifically, we want to compute $\\frac{\\partial y_i}{\\partial z_k}$ for each pair of $i$ and $k$.\n",
    "\n",
    "**Case 1: When $i = k$**\n",
    "\n",
    "Let's start with the derivative of $y_i$ with respect to $z_i$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_i}{\\partial z_i} = \\frac{\\partial}{\\partial z_i} \\left(\\frac{e^{z_i}}{\\sum_{j=1}^{n} e^{z_j}}\\right)\n",
    "$$\n",
    "\n",
    "Using the quotient rule:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_i}{\\partial z_i} = \\frac{ \\left(\\sum_{j=1}^{n} e^{z_j}\\right) \\cdot \\frac{\\partial e^{z_i}}{\\partial z_i} - e^{z_i} \\cdot \\frac{\\partial \\sum_{j=1}^{n} e^{z_j}}{\\partial z_i}}{\\left(\\sum_{j=1}^{n} e^{z_j}\\right)^2}\n",
    "$$\n",
    "\n",
    "Since $\\frac{\\partial e^{z_i}}{\\partial z_i} = e^{z_i}$ and $\\frac{\\partial \\sum_{j=1}^{n} e^{z_j}}{\\partial z_i} = e^{z_i}$, we have:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_i}{\\partial z_i} = \\frac{e^{z_i} \\sum_{j=1}^{n} e^{z_j} - e^{z_i} \\cdot e^{z_i}}{\\left(\\sum_{j=1}^{n} e^{z_j}\\right)^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_i}{\\partial z_i} = \\frac{e^{z_i} \\left(\\sum_{j=1}^{n} e^{z_j} - e^{z_i}\\right)}{\\left(\\sum_{j=1}^{n} e^{z_j}\\right)^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_i}{\\partial z_i} = \\frac{e^{z_i}}{\\sum_{j=1}^{n} e^{z_j}} \\cdot \\left(1 - \\frac{e^{z_i}}{\\sum_{j=1}^{n} e^{z_j}}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_i}{\\partial z_i} = y_i \\cdot (1 - y_i)\n",
    "$$\n",
    "\n",
    "**Case 2: When $i \\neq k$**\n",
    "\n",
    "Now, consider the derivative of $y_i$ with respect to $z_k$ where $i \\neq k$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_i}{\\partial z_k} = \\frac{\\partial}{\\partial z_k} \\left(\\frac{e^{z_i}}{\\sum_{j=1}^{n} e^{z_j}}\\right)\n",
    "$$\n",
    "\n",
    "Again, using the quotient rule:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_i}{\\partial z_k} = \\frac{0 \\cdot \\sum_{j=1}^{n} e^{z_j} - e^{z_i} \\cdot \\frac{\\partial \\sum_{j=1}^{n} e^{z_j}}{\\partial z_k}}{\\left(\\sum_{j=1}^{n} e^{z_j}\\right)^2}\n",
    "$$\n",
    "\n",
    "Since $\\frac{\\partial \\sum_{j=1}^{n} e^{z_j}}{\\partial z_k} = e^{z_k}$, we have:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_i}{\\partial z_k} = - \\frac{e^{z_i} \\cdot e^{z_k}}{\\left(\\sum_{j=1}^{n} e^{z_j}\\right)^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_i}{\\partial z_k} = - \\frac{e^{z_i}}{\\sum_{j=1}^{n} e^{z_j}} \\cdot \\frac{e^{z_k}}{\\sum_{j=1}^{n} e^{z_j}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_i}{\\partial z_k} = - y_i \\cdot y_k\n",
    "$$\n",
    "\n",
    "### Putting It All Together\n",
    "\n",
    "The gradient of the softmax function with respect to its inputs can be expressed as:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_i}{\\partial z_k} = \n",
    "\\begin{cases}\n",
    "y_i \\cdot (1 - y_i) & \\text{if } i = k \\\\\n",
    "- y_i \\cdot y_k & \\text{if } i \\neq k\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This result can also be written more compactly using the Kronecker delta notation $\\delta_{ik}$, which is 1 if $i = k$ and 0 otherwise:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_i}{\\partial z_k} = y_i \\cdot (\\delta_{ik} - y_k)\n",
    "$$\n",
    "\n",
    "This is the gradient of the softmax function with respect to its inputs, and it's crucial for calculating gradients in neural networks during backpropagation when using softmax as the output activation function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Gradient Flow in Classification Problems\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In a typical classification problem, we have:\n",
    "\n",
    "- **N**: The number of data points in the dataset.\n",
    "- **C**: The number of classes or categories the model can predict.\n",
    "\n",
    "For each data point \\( $ x_i $ \\), the model outputs a probability distribution over the **C** classes using a softmax layer. The training process involves minimizing a loss function, often the cross-entropy loss, and updating the model's parameters using gradient descent.\n",
    "\n",
    "## Cross-Entropy Loss Function\n",
    "\n",
    "Given the true label \\( $ y_i $ \\) (one-hot encoded) and the predicted probability \\( $ \\hat{y}_i $ \\) from the softmax layer, the cross-entropy loss for a single data point is:\n",
    "\n",
    "$$\n",
    "L_i = -\\sum_{j=1}^{C} y_{ij} \\log(\\hat{y}_{ij})\n",
    "$$\n",
    "\n",
    "The total loss over all **N** data points is:\n",
    "\n",
    "$$\n",
    "L = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{C} y_{ij} \\log(\\hat{y}_{ij})\n",
    "$$\n",
    "\n",
    "## Softmax Output\n",
    "\n",
    "The softmax function converts the logits (raw output of the network) \\( $ z_i $ \\) into probabilities \\( $ \\hat{y}_i $ \\) for each class:\n",
    "\n",
    "$$\n",
    "\\hat{y}_{ij} = \\frac{e^{z_{ij}}}{\\sum_{k=1}^{C} e^{z_{ik}}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( $ z_{ij }$ \\) is the logit for the \\( $ j $ \\)-th class of the \\( i \\)-th data point.\n",
    "- \\( $\\hat{y}_{ij}$ \\) is the predicted probability of the \\( i \\)-th data point belonging to class \\( j \\).\n",
    "\n",
    "## Gradient Flow\n",
    "\n",
    "### Gradient with Respect to the Softmax Output\n",
    "\n",
    "The gradient of the loss function with respect to the softmax output \\( $\\hat{y}_{ij}$ \\) is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\hat{y}_{ij}} = -\\frac{y_{ij}}{\\hat{y}_{ij}}\n",
    "$$\n",
    "\n",
    "### Gradient with Respect to the Logits\n",
    "\n",
    "To compute the gradient with respect to the logits \\( $ z_{ij} $ \\), we apply the chain rule. The key steps are:\n",
    "\n",
    "1. **Softmax Gradient**:\n",
    "\n",
    "   - For \\( i = j \\):\n",
    "\n",
    "     $$\n",
    "     \\frac{\\partial \\hat{y}_{ij}}{\\partial z_{ij}} = \\hat{y}_{ij}(1 - \\hat{y}_{ij})\n",
    "     $$\n",
    "\n",
    "   - For \\( $i \\neq j $\\):\n",
    "\n",
    "     $$\n",
    "     \\frac{\\partial \\hat{y}_{ik}}{\\partial z_{ij}} = -\\hat{y}_{ij}\\hat{y}_{ik}\n",
    "     $$\n",
    "\n",
    "2. **Chain Rule Application**:\n",
    "\n",
    "   The gradient of the loss with respect to the logits is:\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial z_{ij}} = \\hat{y}_{ij} - y_{ij}\n",
    "   $$\n",
    "\n",
    "   This result shows that the gradient is the difference between the predicted probability \\(  $ \\hat{y}_{ij} $ \\) and the true label \\( $ y_{ij} $ \\).\n",
    "\n",
    "### Gradient with Respect to Weights and Biases\n",
    "\n",
    "Assume the logits \\( $ z_{ij} $ \\) are computed as a linear combination of input features \\( $ x_i $ \\) with weights \\( $ w_j $ \\) and bias \\( $ b_j $ \\):\n",
    "\n",
    "$$\n",
    "z_{ij} = w_j^T x_i + b_j\n",
    "$$\n",
    "\n",
    "The gradients with respect to the weights and biases are:\n",
    "\n",
    "- **Weights**:\n",
    "\n",
    "  $$\n",
    "  \\frac{\\partial L}{\\partial w_j} = \\sum_{i=1}^{N} \\frac{\\partial L}{\\partial z_{ij}} \\cdot \\frac{\\partial z_{ij}}{\\partial w_j} = \\sum_{i=1}^{N} (\\hat{y}_{ij} - y_{ij}) x_i\n",
    "  $$\n",
    "\n",
    "- **Biases**:\n",
    "\n",
    "  $$\n",
    "  \\frac{\\partial L}{\\partial b_j} = \\sum_{i=1}^{N} (\\hat{y}_{ij} - y_{ij})\n",
    "  $$\n",
    "\n",
    "These gradients are then used to update the model's parameters during the training process using gradient descent or a variant like stochastic gradient descent (SGD).\n",
    "\n",
    "## Summary\n",
    "\n",
    "- The gradient of the loss with respect to the logits \\( $ z_{ij} $ \\) is simply the difference between the predicted probability and the true label.\n",
    "- The gradients with respect to the weights and biases are derived using the chain rule and are used in the optimization process to minimize the loss function and improve the model's performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(10)\n",
    "def sigmoid_f(x: torch.Tensor) -> torch.Tensor:\n",
    "    return 1 / (1 + torch.exp(-x))\n",
    "def softmax_f(x: torch.Tensor) -> torch.Tensor:\n",
    "    exp_x = torch.exp(x)\n",
    "    return exp_x / exp_x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5274, 0.6246, 0.5872, 0.5913, 0.5987, 0.5771, 0.5442, 0.5708, 0.5868,\n",
      "        0.5979])\n"
     ]
    }
   ],
   "source": [
    "print(sigmoid_f(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2293, 0.2044, 0.2483, 0.2175, 0.2471, 0.2340, 0.2267, 0.2224, 0.2499,\n",
      "        0.2470])\n"
     ]
    }
   ],
   "source": [
    "print(sigmoid_f(x) * (1- sigmoid_f(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0967, 0.1271, 0.0657, 0.1113, 0.0689, 0.0904, 0.1000, 0.1053, 0.0581,\n",
      "        0.0690])\n"
     ]
    }
   ],
   "source": [
    "print(softmax_f(x) * (1- softmax_f(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0118, 0.0162, 0.0077, 0.0138, 0.0081, 0.0109, 0.0122, 0.0130, 0.0067,\n",
       "         0.0081],\n",
       "        [0.0162, 0.0223, 0.0106, 0.0191, 0.0111, 0.0150, 0.0168, 0.0179, 0.0093,\n",
       "         0.0111],\n",
       "        [0.0077, 0.0106, 0.0050, 0.0090, 0.0053, 0.0071, 0.0080, 0.0085, 0.0044,\n",
       "         0.0053],\n",
       "        [0.0138, 0.0191, 0.0090, 0.0163, 0.0095, 0.0128, 0.0144, 0.0153, 0.0079,\n",
       "         0.0095],\n",
       "        [0.0081, 0.0111, 0.0053, 0.0095, 0.0055, 0.0075, 0.0084, 0.0089, 0.0046,\n",
       "         0.0056],\n",
       "        [0.0109, 0.0150, 0.0071, 0.0128, 0.0075, 0.0101, 0.0113, 0.0120, 0.0062,\n",
       "         0.0075],\n",
       "        [0.0122, 0.0168, 0.0080, 0.0144, 0.0084, 0.0113, 0.0127, 0.0135, 0.0070,\n",
       "         0.0084],\n",
       "        [0.0130, 0.0179, 0.0085, 0.0153, 0.0089, 0.0120, 0.0135, 0.0143, 0.0074,\n",
       "         0.0089],\n",
       "        [0.0067, 0.0093, 0.0044, 0.0079, 0.0046, 0.0062, 0.0070, 0.0074, 0.0038,\n",
       "         0.0046],\n",
       "        [0.0081, 0.0111, 0.0053, 0.0095, 0.0056, 0.0075, 0.0084, 0.0089, 0.0046,\n",
       "         0.0056]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.outer(softmax_f(x), softmax_f(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.2293, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000],\n",
       "         [0.0000, 0.2044, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000],\n",
       "         [0.0000, 0.0000, 0.2483, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.2175, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.2471, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2340, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2267, 0.0000, 0.0000,\n",
       "          0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2224, 0.0000,\n",
       "          0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2499,\n",
       "          0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.2470]]),\n",
       " tensor([[ 0.0967, -0.0162, -0.0077, -0.0138, -0.0081, -0.0109, -0.0122, -0.0130,\n",
       "          -0.0067, -0.0081],\n",
       "         [-0.0162,  0.1271, -0.0106, -0.0191, -0.0111, -0.0150, -0.0168, -0.0179,\n",
       "          -0.0093, -0.0111],\n",
       "         [-0.0077, -0.0106,  0.0657, -0.0090, -0.0053, -0.0071, -0.0080, -0.0085,\n",
       "          -0.0044, -0.0053],\n",
       "         [-0.0138, -0.0191, -0.0090,  0.1113, -0.0095, -0.0128, -0.0144, -0.0153,\n",
       "          -0.0079, -0.0095],\n",
       "         [-0.0081, -0.0111, -0.0053, -0.0095,  0.0689, -0.0075, -0.0084, -0.0089,\n",
       "          -0.0046, -0.0056],\n",
       "         [-0.0109, -0.0150, -0.0071, -0.0128, -0.0075,  0.0904, -0.0113, -0.0120,\n",
       "          -0.0062, -0.0075],\n",
       "         [-0.0122, -0.0168, -0.0080, -0.0144, -0.0084, -0.0113,  0.1000, -0.0135,\n",
       "          -0.0070, -0.0084],\n",
       "         [-0.0130, -0.0179, -0.0085, -0.0153, -0.0089, -0.0120, -0.0135,  0.1053,\n",
       "          -0.0074, -0.0089],\n",
       "         [-0.0067, -0.0093, -0.0044, -0.0079, -0.0046, -0.0062, -0.0070, -0.0074,\n",
       "           0.0581, -0.0046],\n",
       "         [-0.0081, -0.0111, -0.0053, -0.0095, -0.0056, -0.0075, -0.0084, -0.0089,\n",
       "          -0.0046,  0.0690]]))"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.autograd.functional import jacobian, hessian\n",
    "from torch.autograd import grad\n",
    "\n",
    "jacobian(sigmoid_f, x), jacobian(softmax_f, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we will see how local gradient is calculated for linear transformations of neural networks.\n",
    "\n",
    "### Formulation of the Problem\n",
    "\n",
    "Given a weight matrix \\( $W$ \\) and a bias vector \\( $b$ \\), the output of a linear transformation is given by:\n",
    "\n",
    "$$\n",
    "z = Wx + b\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\( $W$ \\) is a \\( $ m \\times n $ \\) matrix.\n",
    "- \\( $x$ \\) is a \\($  n \\times 1 $ \\) column vector.\n",
    "- \\( $b$ \\) is a \\( $ m \\times 1 $\\) column vector.\n",
    "- \\( $z$ \\) is a \\( $ m \\times 1 $\\) column vector.\n",
    "\n",
    "At this point we should see matrix as a transformation of space. The matrix \\( $W$ \\) transforms the input vector \\( $x$ \\) from an \\( $ n $ \\)-dimensional space to an \\( $ m $ \\)-dimensional space. The bias vector \\( $b$ \\) shifts the transformed vector in the \\( $ m $ \\)-dimensional space. The columns of the matrix \\( $W$ \\) are the directions in which the input vector \\( $x$ \\) is transformed. A matrix multiplication of a vector with the matrix \\( $W$ \\) is a linear combination of the columns of the matrix \\( $W$ \\) with the vector \\( $x$ \\) as the coefficients.\n",
    "\n",
    "$$\n",
    "W = \\begin{bmatrix} | & | & | \\\\ w_1 & w_2 & w_3 \\\\ | & | & | \\end{bmatrix} \\\\\n",
    "W. \\vec{x} = x1 * \\vec{w_1} + x2 * \\vec{w_2} + x3 * \\vec{w_3}\n",
    "$$\n",
    "\n",
    "In terms of basis vectors, the matrix \\( $W$ \\) transforms the standard basis vectors \\( $\\vec{e_1},\\vec{e_2}, \\ldots, \\vec{e_n} $ \\) to the columns of the matrix \\( $W$ \\). The output of the linear transformation is a linear combination of the transformed basis vectors.\n",
    "\n",
    "### Local Gradient\n",
    "\n",
    "Lets derive the local gradient of linear transformation. This is a vector valued transformation. For a vector valued transformation, the local gradient is a Jacobian matrix. The Jacobian matrix is a matrix of partial derivatives of the output vector with respect to the input vector. Computing the Jacobian matrix involves computing the partial derivatives of each output element with respect to each input element.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "z_i & = \\sum_{j=1}^{n} W_{ij}x_j + b_i \\\\\n",
    "\\frac{\\partial z_i}{\\partial x_j} & = W_{ij} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The Jacobian matrix is given by:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "J & = \\begin{bmatrix} \\frac{\\partial z_1}{\\partial x_1} & \\frac{\\partial z_1}{\\partial x_2} & \\ldots & \\frac{\\partial z_1}{\\partial x_n} \\\\ \\frac{\\partial z_2}{\\partial x_1} & \\frac{\\partial z_2}{\\partial x_2} & \\ldots & \\frac{\\partial z_2}{\\partial x_n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial z_m}{\\partial x_1} & \\frac{\\partial z_m}{\\partial x_2} & \\ldots & \\frac{\\partial z_m}{\\partial x_n} \\end{bmatrix} \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
