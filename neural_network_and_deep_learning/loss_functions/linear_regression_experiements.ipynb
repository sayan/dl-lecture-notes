{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import statsmodels.api as sm\n",
    "from torch.nn import MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generation\n",
    "num_features = 5\n",
    "num_data_points = 5000\n",
    "x = torch.rand(num_data_points, num_features)\n",
    "true_weights = torch.randint(5, 10, size=(1, num_features)).float()\n",
    "true_bias = torch.randint(5, 10, size=(1,)).float()\n",
    "error = torch.randn((num_data_points, 1))\n",
    "y_target = true_bias + x @ true_weights.T + error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>ols_weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>5.905957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.0</td>\n",
       "      <td>8.019606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.0</td>\n",
       "      <td>7.005886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.896109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.0</td>\n",
       "      <td>7.034547</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   actual  ols_weights\n",
       "0     6.0     5.905957\n",
       "1     8.0     8.019606\n",
       "2     7.0     7.005886\n",
       "3     5.0     4.896109\n",
       "4     7.0     7.034547"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert PyTorch tensors to NumPy arrays for use with statsmodels\n",
    "import pandas as pd\n",
    "x_np = x.detach().numpy()\n",
    "y_target_np = y_target.detach().numpy().squeeze()\n",
    "\n",
    "# Add a column of ones to X for the intercept (bias)\n",
    "x_np_with_intercept = sm.add_constant(x_np)\n",
    "\n",
    "# Fit the OLS model\n",
    "ols_model = sm.OLS(y_target_np, x_np_with_intercept).fit()\n",
    "\n",
    "# Extract the parameter estimates from OLS\n",
    "ols_weights = ols_model.params[1:]  # Coefficients (weights)\n",
    "ols_bias = ols_model.params[0]  # Intercept (bias)\n",
    "\n",
    "params_df = pd.DataFrame({\n",
    "    'actual': true_weights.detach().numpy().squeeze(),\n",
    "    'ols_weights': ols_weights\n",
    "})\n",
    "params_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight shape tensor([[ 0.5978, -0.5340, -0.7392,  0.4005,  0.4270]], requires_grad=True)\n",
      "Bias Shape tensor([0.4175], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "## now we do with back propagation.  We initialise the weights and biases\n",
    "weight_initial = torch.randn((1, num_features), requires_grad=True)\n",
    "bias_initial = torch.randn(1, requires_grad=True)\n",
    "print(f\"Weight shape {weight_initial}\")\n",
    "print(f\"Bias Shape {bias_initial}\")\n",
    "batch_size = 128\n",
    "lr = 0.001\n",
    "epochs = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of current batch: 1.1736273765563965\n",
      "Loss of current batch: 1.0449539422988892\n",
      "Loss of current batch: 1.0110138654708862\n",
      "Loss of current batch: 0.918136715888977\n",
      "Loss of current batch: 0.9421952366828918\n",
      "Loss of current batch: 0.9346218705177307\n",
      "Loss of current batch: 1.1740509271621704\n",
      "Loss of current batch: 0.9785818457603455\n",
      "Loss of current batch: 0.7621504068374634\n",
      "Loss of current batch: 0.9516388177871704\n",
      "Loss of current batch: 0.979264497756958\n",
      "Loss of current batch: 1.201424241065979\n",
      "Loss of current batch: 0.9267152547836304\n",
      "Loss of current batch: 0.9184979200363159\n",
      "Loss of current batch: 0.8523565530776978\n",
      "Loss of current batch: 0.9455530047416687\n",
      "Loss of current batch: 0.8586906790733337\n",
      "Loss of current batch: 1.1558098793029785\n",
      "Loss of current batch: 1.1044906377792358\n",
      "Loss of current batch: 1.0794744491577148\n",
      "Loss of current batch: 1.093479871749878\n",
      "Loss of current batch: 0.9180593490600586\n",
      "Loss of current batch: 0.8200418949127197\n",
      "Loss of current batch: 1.01742422580719\n",
      "Loss of current batch: 0.9669516682624817\n",
      "Loss of current batch: 1.1318447589874268\n",
      "Loss of current batch: 0.9912722706794739\n",
      "Loss of current batch: 1.1452139616012573\n",
      "Loss of current batch: 0.9597550630569458\n",
      "Loss of current batch: 0.9638813734054565\n",
      "Loss of current batch: 0.9355881214141846\n",
      "Loss of current batch: 1.0180189609527588\n",
      "Loss of current batch: 1.215448260307312\n",
      "Loss of current batch: 0.9600683450698853\n",
      "Loss of current batch: 0.9758068919181824\n",
      "Loss of current batch: 0.8428679704666138\n",
      "Loss of current batch: 1.2170507907867432\n",
      "Loss of current batch: 0.9087121486663818\n",
      "Loss of current batch: 0.8977144956588745\n",
      "Loss of current batch: 0.8180350065231323\n",
      "Loss of current batch: 0.8894826769828796\n",
      "Loss of current batch: 0.9017341732978821\n",
      "Loss of current batch: 1.0723333358764648\n",
      "Loss of current batch: 0.9999165534973145\n",
      "Loss of current batch: 1.1080589294433594\n",
      "Loss of current batch: 0.9519966840744019\n",
      "Loss of current batch: 1.1300214529037476\n",
      "Loss of current batch: 1.050511360168457\n",
      "Loss of current batch: 0.9707171320915222\n",
      "Loss of current batch: 1.0529708862304688\n",
      "Loss of current batch: 1.2495203018188477\n",
      "Loss of current batch: 1.154937982559204\n",
      "Loss of current batch: 0.7919207215309143\n",
      "Loss of current batch: 1.0744848251342773\n",
      "Loss of current batch: 1.0426667928695679\n",
      "Loss of current batch: 0.7774227261543274\n",
      "Loss of current batch: 0.890717089176178\n",
      "Loss of current batch: 1.0112106800079346\n",
      "Loss of current batch: 0.937703549861908\n",
      "Loss of current batch: 0.9243215918540955\n",
      "Loss of current batch: 0.838238537311554\n",
      "Loss of current batch: 1.2918914556503296\n",
      "Loss of current batch: 1.0496888160705566\n",
      "Loss of current batch: 0.9112904667854309\n",
      "Loss of current batch: 0.8676015138626099\n",
      "Loss of current batch: 0.9596915245056152\n",
      "Loss of current batch: 1.0185587406158447\n",
      "Loss of current batch: 1.0030208826065063\n",
      "Loss of current batch: 1.0968064069747925\n",
      "Loss of current batch: 0.9783222079277039\n",
      "Loss of current batch: 0.9673889875411987\n",
      "Loss of current batch: 0.8404569625854492\n",
      "Loss of current batch: 0.7904829382896423\n",
      "Loss of current batch: 0.9718825817108154\n",
      "Loss of current batch: 0.6919621825218201\n",
      "Loss of current batch: 0.9946128726005554\n",
      "Loss of current batch: 0.9539390802383423\n",
      "Loss of current batch: 0.9036836624145508\n",
      "Loss of current batch: 1.0062116384506226\n",
      "Loss of current batch: 0.9240313768386841\n",
      "Loss of current batch: 0.8660900592803955\n",
      "Loss of current batch: 0.9532408118247986\n",
      "Loss of current batch: 1.0491948127746582\n",
      "Loss of current batch: 0.8396269083023071\n",
      "Loss of current batch: 1.2070202827453613\n",
      "Loss of current batch: 1.0253361463546753\n",
      "Loss of current batch: 1.1039087772369385\n",
      "Loss of current batch: 1.1801955699920654\n",
      "Loss of current batch: 1.0795557498931885\n",
      "Loss of current batch: 0.8022153973579407\n",
      "Loss of current batch: 1.0020464658737183\n",
      "Loss of current batch: 0.9387749433517456\n",
      "Loss of current batch: 1.2129335403442383\n",
      "Loss of current batch: 1.2284773588180542\n",
      "Loss of current batch: 1.053536295890808\n",
      "Loss of current batch: 1.0832324028015137\n",
      "Loss of current batch: 1.262948989868164\n",
      "Loss of current batch: 0.9855645895004272\n",
      "Loss of current batch: 1.2478841543197632\n",
      "Loss of current batch: 0.9035522937774658\n",
      "Loss of current batch: 0.8551918268203735\n",
      "Loss of current batch: 1.0850008726119995\n",
      "Loss of current batch: 0.9989666938781738\n",
      "Loss of current batch: 1.055168628692627\n",
      "Loss of current batch: 0.9629789590835571\n",
      "Loss of current batch: 0.9605503082275391\n",
      "Loss of current batch: 0.7730897665023804\n",
      "Loss of current batch: 0.8974212408065796\n",
      "Loss of current batch: 1.1762986183166504\n",
      "Loss of current batch: 1.0447196960449219\n",
      "Loss of current batch: 0.8396804928779602\n",
      "Loss of current batch: 1.0075581073760986\n",
      "Loss of current batch: 0.8826708197593689\n",
      "Loss of current batch: 1.071638822555542\n",
      "Loss of current batch: 0.9270950555801392\n",
      "Loss of current batch: 0.8564994931221008\n",
      "Loss of current batch: 1.148585557937622\n",
      "Loss of current batch: 0.949246883392334\n",
      "Loss of current batch: 0.9687985181808472\n",
      "Loss of current batch: 0.9305702447891235\n",
      "Loss of current batch: 1.029452919960022\n",
      "Loss of current batch: 0.8234412670135498\n",
      "Loss of current batch: 0.7403402328491211\n",
      "Loss of current batch: 0.9845422506332397\n",
      "Loss of current batch: 0.8809289932250977\n",
      "Loss of current batch: 0.7064962387084961\n",
      "Loss of current batch: 1.00501549243927\n",
      "Loss of current batch: 0.8767229914665222\n",
      "Loss of current batch: 1.0544909238815308\n",
      "Loss of current batch: 0.987028181552887\n",
      "Loss of current batch: 0.962996244430542\n",
      "Loss of current batch: 1.077095627784729\n",
      "Loss of current batch: 0.9419971704483032\n",
      "Loss of current batch: 1.1511194705963135\n",
      "Loss of current batch: 1.0174318552017212\n",
      "Loss of current batch: 1.191165566444397\n",
      "Loss of current batch: 0.8807278275489807\n",
      "Loss of current batch: 0.8405792117118835\n",
      "Loss of current batch: 0.7984496355056763\n",
      "Loss of current batch: 0.9585612416267395\n",
      "Loss of current batch: 1.1490519046783447\n",
      "Loss of current batch: 0.8566809892654419\n",
      "Loss of current batch: 0.9359200596809387\n",
      "Loss of current batch: 1.0055738687515259\n",
      "Loss of current batch: 0.9285500049591064\n",
      "Loss of current batch: 0.9289231300354004\n",
      "Loss of current batch: 1.0515459775924683\n",
      "Loss of current batch: 0.9796623587608337\n",
      "Loss of current batch: 0.9162893295288086\n",
      "Loss of current batch: 1.1039080619812012\n",
      "Loss of current batch: 1.024591326713562\n",
      "Loss of current batch: 0.8324219584465027\n",
      "Loss of current batch: 0.8050968050956726\n",
      "Loss of current batch: 1.0410897731781006\n",
      "Loss of current batch: 1.0265696048736572\n",
      "Loss of current batch: 1.1903809309005737\n",
      "Loss of current batch: 0.836004376411438\n",
      "Loss of current batch: 1.002671241760254\n",
      "Loss of current batch: 0.9198011159896851\n",
      "Loss of current batch: 0.9285502433776855\n",
      "Loss of current batch: 1.1621458530426025\n",
      "Loss of current batch: 0.8117924332618713\n",
      "Loss of current batch: 0.9032588005065918\n",
      "Loss of current batch: 1.1827174425125122\n",
      "Loss of current batch: 1.268977165222168\n",
      "Loss of current batch: 1.1153278350830078\n",
      "Loss of current batch: 1.084926962852478\n",
      "Loss of current batch: 0.9206682443618774\n",
      "Loss of current batch: 1.0241131782531738\n",
      "Loss of current batch: 0.9669072031974792\n",
      "Loss of current batch: 0.8890215754508972\n",
      "Loss of current batch: 0.9572263956069946\n",
      "Loss of current batch: 0.8199939131736755\n",
      "Loss of current batch: 0.8512870669364929\n",
      "Loss of current batch: 0.9154466390609741\n",
      "Loss of current batch: 0.9400613307952881\n",
      "Loss of current batch: 0.9505027532577515\n",
      "Loss of current batch: 0.863848865032196\n",
      "Loss of current batch: 0.9494838714599609\n",
      "Loss of current batch: 0.9721605181694031\n",
      "Loss of current batch: 0.9196458458900452\n",
      "Loss of current batch: 1.1707056760787964\n",
      "Loss of current batch: 1.0745279788970947\n",
      "Loss of current batch: 0.9514197111129761\n",
      "Loss of current batch: 1.029186487197876\n",
      "Loss of current batch: 1.0760564804077148\n",
      "Loss of current batch: 1.0266138315200806\n",
      "Loss of current batch: 1.0030713081359863\n",
      "Loss of current batch: 1.1430232524871826\n",
      "Loss of current batch: 0.9658522605895996\n",
      "Loss of current batch: 0.9753378033638\n",
      "Loss of current batch: 1.12251877784729\n",
      "Loss of current batch: 0.8050628900527954\n",
      "Loss of current batch: 0.9610743522644043\n",
      "Loss of current batch: 1.1491836309432983\n",
      "Loss of current batch: 1.0865120887756348\n",
      "Loss of current batch: 0.8738085031509399\n",
      "Loss of current batch: 0.8855916857719421\n",
      "Loss of current batch: 0.8522366881370544\n",
      "Loss of current batch: 1.0978556871414185\n"
     ]
    }
   ],
   "source": [
    "uniform_probs = torch.ones(num_data_points) / num_data_points  # Uniform probabilities over 10 outcomes\n",
    "dist = torch.distributions.Categorical(probs=uniform_probs)\n",
    "mse_loss_f = MSELoss()\n",
    "\n",
    "for cur_epoch in range(epochs):\n",
    "    ## do the forward pass\n",
    "    cur_batch_index = dist.sample((batch_size,))\n",
    "    cur_batch_x = x[cur_batch_index,]\n",
    "    cur_batch_y = y_target[cur_batch_index,]\n",
    "    #print(cur_batch_x[:2,])\n",
    "    cur_batch_predict_y = cur_batch_x @ weight_initial.T + bias_initial\n",
    "    cur_batch_loss = mse_loss_f(cur_batch_predict_y,cur_batch_y )\n",
    "    if cur_epoch%500 == 0:\n",
    "        print(f\"Loss of current batch: {cur_batch_loss}\")\n",
    "    cur_batch_loss.backward()\n",
    "    with torch.no_grad():\n",
    "        weight_initial -= lr*weight_initial.grad\n",
    "        bias_initial -=  lr * bias_initial.grad\n",
    "    ## make zero grad to avoide gradient accumlation\n",
    "    weight_initial.grad.zero_()\n",
    "    bias_initial.grad.zero_()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[5.9048, 8.0205, 7.0086, 4.8951, 7.0362]], requires_grad=True),\n",
       " tensor([9.0788], requires_grad=True))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_initial, bias_initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>ols_weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>5.905957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.0</td>\n",
       "      <td>8.019606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.0</td>\n",
       "      <td>7.005886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.896109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.0</td>\n",
       "      <td>7.034547</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   actual  ols_weights\n",
       "0     6.0     5.905957\n",
       "1     8.0     8.019606\n",
       "2     7.0     7.005886\n",
       "3     5.0     4.896109\n",
       "4     7.0     7.034547"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
